![header](https://capsule-render.vercel.app/api?type=speech&color=gradient&height=200&section=header&text=2025-2%20AI%2BX%20DeepLearning&desc=BERT%20ê¸°ë°˜%20í•œêµ­ì–´%20ê°ì„±%20ë¶„ì„Â·í‚¤ì›Œë“œ%20ì¶”ì¶œì„%20ì´ìš©í•œ%20GIF%20ì¶”ì²œ%20ì‹œìŠ¤í…œ%20ê°œë°œ&fontSize=50&fontColor=FFFFFF&fontAlignY=35)
# BERT ê¸°ë°˜ í•œêµ­ì–´ ê°ì„± ë¶„ì„Â·í‚¤ì›Œë“œ ì¶”ì¶œì„ ì´ìš©í•œ GIF ì¶”ì²œ ì‹œìŠ¤í…œ ê°œë°œ

- **íŒ€ì›1 - í•œì–‘ëŒ€í•™êµ ê²½ì˜í•™ë¶€ ë°•ì„±í—Œ(2018027074)**
    - KoBERT ëª¨ë¸ í•™ìŠµ ë° í‰ê°€
    - GIF ì¶”ì²œ ì‹œìŠ¤í…œ ì„¤ê³„
    - Streamlit êµ¬í˜„
    - ë³´ê³ ì„œ ì‘ì„± ë° ì˜ìƒ ë…¹í™”

- **íŒ€ì›2 - í•œì–‘ëŒ€í•™êµ ê²½ì˜í•™ë¶€ ì¸ìš©ê±´(2019078122)**
    - ë°ì´í„° ì „ì²˜ë¦¬
    - KoELECTRA ëª¨ë¸ í•™ìŠµ ë° í‰ê°€
    - GIF ì¶”ì²œ ì‹œìŠ¤í…œ ì„¤ê³„
    - ë³´ê³ ì„œ ì‘ì„± ë° ì˜ìƒ ë…¹í™”

- **Video Link** (https://youtu.be/ufOfsBKDpxA?si=PSigJPS08zr62-TN)


## 0. í”„ë¡œì íŠ¸ ê°œìš”
### í”„ë¡œì íŠ¸ ì†Œê°œ
ë³¸ í”„ë¡œì íŠ¸ëŠ” **KoBERT/KoELECTRA ê¸°ë°˜ í•œêµ­ì–´ ê°ì • ë¶„ë¥˜ ëª¨ë¸**ê³¼ **í‚¤ì›Œë“œ ì¶”ì¶œ ë¡œì§**ì„ ê²°í•©í•˜ì—¬, ë©”ì‹ ì € ì‚¬ìš©ìì˜ ì…ë ¥ ë¬¸ì¥ì— ì–´ìš¸ë¦¬ëŠ” **GIFë¥¼ ìë™ìœ¼ë¡œ ì¶”ì²œí•´ì£¼ëŠ” ì‹œìŠ¤í…œ**ì„ êµ¬í˜„í•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í–ˆë‹¤.

<p align="center">
  <img src="images/preview 1.png">
</p>
<p align="center">
  <img src="images/preview 2.png">
</p>

### ì£¼ì œ ì„ ì • ë°°ê²½
ì¹´ì¹´ì˜¤í†¡, ì¸ìŠ¤íƒ€ê·¸ë¨ê³¼ ê°™ì€ í”Œë«í¼ ë¹„ì¦ˆë‹ˆìŠ¤ëŠ” ì‚¬ìš©ìë“¤ì´ ì„œë¹„ìŠ¤ ì•ˆì— ë” ì˜¤ë˜ ë¨¸ë¬¼ê³ , ë” ìì£¼ ìƒí˜¸ì‘ìš©í•˜ë„ë¡ ë§Œë“œëŠ” ê²ƒì„ í•µì‹¬ ëª©í‘œë¡œ ì‚¼ëŠ”ë‹¤. ì´ëŸ¬í•œ í™˜ê²½ì—ì„œ ì‚¬ìš©ìëŠ” í…ìŠ¤íŠ¸ë¥¼ ì´ìš©í•œ ì–¸ì–´ì  ì†Œí†µë¿ë§Œ ì•„ë‹ˆë¼, ì´ëª¨í‹°ì½˜Â·ìŠ¤í‹°ì»¤Â·GIFì™€ ê°™ì€ ë¹„ì–¸ì–´ì  í‘œí˜„ ìˆ˜ë‹¨ë„ í™œë°œí•˜ê²Œ ì‚¬ìš©í•œë‹¤. ìµœê·¼ ì¹´ì¹´ì˜¤í†¡ì— ë„ì…ëœ ì´ëª¨í‹°ì½˜ ì¶”ì²œ ê¸°ëŠ¥, ì¸ìŠ¤íƒ€ê·¸ë¨ ë‹¤ì´ë ‰íŠ¸ ë©”ì‹œì§€(DM)ì˜ GIPHY ì—°ë™ ê¸°ëŠ¥ì€ ì´ëŸ¬í•œ íë¦„ì„ ì˜ ë³´ì—¬ì£¼ëŠ” ëŒ€í‘œì  ì‚¬ë¡€ë‹¤.

ì¹´ì¹´ì˜¤í†¡ì˜ ì´ëª¨í‹°ì½˜ ì¶”ì²œ ê¸°ëŠ¥ì€ ì‚¬ìš©ìê°€ ì…ë ¥í•œ ë¬¸ì¥ì„ ë¶„ì„í•´ ì´ëª¨í‹°ì½˜ì„ ì¶”ì²œí•´ ì¤€ë‹¤ëŠ” ì ì—ì„œ í¸ì˜ì„±ê³¼ ëŒ€í™” ëª°ì…ë„ë¥¼ ë†’ì´ëŠ” ì¥ì ì´ ìˆë‹¤. ê·¸ëŸ¬ë‚˜ ì¶”ì²œ ëŒ€ìƒì´ ì¹´ì¹´ì˜¤í†¡ ë‚´ ìœ ë£Œ ì´ëª¨í‹°ì½˜ì— ì œí•œëœë‹¤ëŠ” í•œê³„ê°€ ìˆë‹¤. ì´ë¡œ ì¸í•´, ì‹¤ì œ ì‚¬ìš©ìì˜ ê°ì •ì´ë‚˜ ìƒí™©ì— ë” ì˜ ë§ëŠ” ë‹¤ì–‘í•œ í‘œí˜„ì„ ì¶©ë¶„íˆ í™œìš©í•˜ê¸° ì–´ë µë‹¤.

ë°˜ë©´ ì¸ìŠ¤íƒ€ê·¸ë¨ DMì— ì—°ë™ëœ GIPHYëŠ” ë°©ëŒ€í•œ ì–‘ì˜ GIFë¥¼ ì œê³µí•˜ì§€ë§Œ, í˜„ì¬ëŠ” ì‚¬ìš©ìê°€ ì§ì ‘ í‚¤ì›Œë“œë¥¼ ê²€ìƒ‰í•˜ëŠ” ë°©ì‹ì´ê³  ëŒ€í™” ë‚´ìš©ì´ë‚˜ ê°ì • ìƒíƒœë¥¼ ë¶„ì„í•´ GIFë¥¼ ì¶”ì²œí•´ ì£¼ëŠ” ê¸°ëŠ¥ì€ ë¶€ì¬í•˜ë‹¤. ì¦‰, ì¹´ì¹´ì˜¤í†¡ì€ ì¶”ì²œ ê¸°ëŠ¥ì€ ìˆìœ¼ë‚˜ í‘œí˜„ ìˆ˜ë‹¨ê³¼ ë²”ìœ„ê°€ ì œí•œì ì´ê³ , GIPHYëŠ” í‘œí˜„ ìˆ˜ë‹¨ì€ í’ë¶€í•˜ì§€ë§Œ ì¶”ì²œ ì¸í…”ë¦¬ì „ìŠ¤ê°€ ë¶€ì¡±í•œ ìƒí™©ì¸ ê²ƒì´ë‹¤.

ì—¬ê¸°ì„œ ìš°ë¦¬ íŒ€ì€ ì‚¬ìš©ìì˜ ëŒ€í™”ë¥¼ ì´í•´í•˜ê³  ê·¸ ì†ì— ë‹´ê¸´ ê°ì •ê³¼ í‚¤ì›Œë“œë¥¼ íŒŒì•…í•œ ë’¤, ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì ì ˆí•œ GIFë¥¼ ìë™ìœ¼ë¡œ ì œì•ˆí•´ ì£¼ëŠ” ì‹œìŠ¤í…œì´ ìƒê¸´ë‹¤ë©´ ì¹´ì¹´ì˜¤í†¡ê³¼ ì¸ìŠ¤íƒ€ê·¸ë¨ì´ ê°€ì§„ ì¥ì ì„ ê²°í•©í•˜ë©´ì„œë„ ê¸°ì¡´ í•œê³„ë¥¼ ë³´ì™„í•  ìˆ˜ ìˆì„ ê²ƒì´ë¼ê³  ìƒê°í–ˆë‹¤. ì´ì— ë”°ë¼, ë³¸ í”„ë¡œì íŠ¸ì—ì„œëŠ” BERT/ELECTRA ê¸°ë°˜ ê°ì • ë¶„ì„ê³¼ í‚¤ì›Œë“œ ì¶”ì¶œ ë¡œì§ì„ ë°”íƒ•ìœ¼ë¡œ ë©”ì‹ ì € ëŒ€í™”ì— ì–´ìš¸ë¦¬ëŠ” GIFë¥¼ ì¶”ì²œí•˜ëŠ” ì‹œìŠ¤í…œì„ ì„¤ê³„í•˜ê³  êµ¬í˜„í•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•œë‹¤.

### ë³¸ í”„ë¡œì íŠ¸ì˜ ì°¨ë³„ì 

- **ëŒ€í™” ê¸°ë°˜ ìë™ ì¶”ì²œ**  
  - ì‚¬ìš©ìê°€ ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ë©´, ê°ì • ë¶„ì„+í‚¤ì›Œë“œ ì¶”ì¶œì„ í†µí•´ ìë™ìœ¼ë¡œ GIF ì¶”ì²œ

- **ì„¸ë¶„í™”ëœ ê°ì • ê¸°ë°˜ ì¶”ì²œ**  
  - ê³µí¬Â·ë†€ëŒÂ·ë¶„ë…¸Â·ìŠ¬í””Â·ì¤‘ë¦½Â·í–‰ë³µÂ·í˜ì˜¤ 7ê°€ì§€ ê°ì •ì„ êµ¬ë¶„í•´ ìƒí™©ì— ë” ì˜ ë§ëŠ” GIF ì„ íƒ ê°€ëŠ¥

- **í”Œë«í¼Â·ì½˜í…ì¸  ì œì•½ ì ìŒ**  
  - ë°©ëŒ€í•œ ë¬´ë£Œ GIF ë¦¬ì†ŒìŠ¤ ììœ ë¡­ê²Œ í™œìš© ê°€ëŠ¥

- **í•œêµ­ì–´ ëŒ€í™” íŠ¹í™”ëœ ëª¨ë¸ ì‚¬ìš©**  
  - KoBERT ê¸°ë°˜ìœ¼ë¡œ í•œêµ­ì–´ ë¬¸ì¥ì„ ì´í•´í•˜ê³  ê°ì •ì„ ì˜ˆì¸¡í•˜ëŠ” ëª¨ë¸ ì‚¬ìš©

### ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜

```mermaid
flowchart TB
    A[ì‚¬ìš©ì ì…ë ¥ ë¬¸ì¥] --> B["í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬"]
    B --> C1["ê°ì„± ë¶„ì„ ëª¨ë“ˆ<br/>(KoBERT/KoELECTRA)"]
    B --> C2["í‚¤ì›Œë“œ ì¶”ì¶œ ëª¨ë“ˆ<br/>(Konlpy Okt)"]

    C1 --> D1["ê°ì • ë ˆì´ë¸”<br/>(ê³µí¬/ë†€ëŒ/ë¶„ë…¸/ìŠ¬í””/ì¤‘ë¦½/í–‰ë³µ/í˜ì˜¤)"]
    C2 --> D2["í•µì‹¬ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸"]

    D1 --> E["GIF ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„±<br/>(ê°ì • + í‚¤ì›Œë“œ ì¡°í•©)"]
    D2 --> E

    E --> F["GIF API í˜¸ì¶œ<br/>(GIPHY)"]
    F --> G["í›„ì²˜ë¦¬ ë° ë­í‚¹"]
    G --> H[ì‚¬ìš©ìì—ê²Œ GIF ì¶”ì²œ ê²°ê³¼ ë°˜í™˜]
```

### Quick Start(ì‹¤í–‰ ë°©ë²•)

#### 1) ì €ì¥ì†Œ í´ë¡ 
```bash
git clone https://github.com/qkrjds201/AIX-DL-2025-Fall.git
cd AIX-DL-2025-Fall
```

#### 2) í•„ìš” íŒ¨í‚¤ì§€ ì„¤ì¹˜
```bash
pip install -r requirements.txt
```

#### 3) app.pyì™€ ê°™ì€ ë””ë ‰í† ë¦¬ì— ì•„ë˜ íŒŒì¼ ì¤€ë¹„
- `kobert_emotion_model_state_dict.pt`
- `koelectra_emotion_model_state_dict.pt`
- `keyboard.png`


#### 4) Streamlit ì•± ì‹¤í–‰
```bash
streamlit run app.py
```



## 1. ë°ì´í„° & ì „ì²˜ë¦¬
### ì‚¬ìš©í•œ ë°ì´í„°
- í•œêµ­ì–´ ê°ì • ì •ë³´ê°€ í¬í•¨ëœ ì—°ì†ì  ëŒ€í™” ë°ì´í„°ì…‹
(https://aihub.or.kr/aihubdata/data/view.do?dataSetSn=271)
- ê°ì„± ëŒ€í™” ë§ë­‰ì¹˜
(https://aihub.or.kr/aihubdata/data/view.do?dataSetSn=86)
- í•œêµ­ì–´ í˜ì˜¤ í‘œí˜„ ë°ì´í„°ì…‹
(https://github.com/smilegate-ai/korean_unsmile_dataset)

'í•œêµ­ì–´ ê°ì • ì •ë³´ê°€ í¬í•¨ëœ ì—°ì†ì  ëŒ€í™” ë°ì´í„°ì…‹'ë§Œìœ¼ë¡œ ê°ì • ë¶„ë¥˜ ëª¨ë¸ì„ í•™ìŠµí•˜ë ¤ê³  í–ˆìœ¼ë‚˜, ì•„ë˜ì™€ ê°™ì´ íŠ¹ì • ê°ì •ì— ë°ì´í„°ê°€ ì ë ¤ ìˆëŠ” í´ë˜ìŠ¤ ë¶ˆê· í˜•ì´ ìˆì–´ ì¶”ê°€ ë°ì´í„°(ê°ì„± ëŒ€í™” ë§ë­‰ì¹˜, í˜ì˜¤ í‘œí˜„ ë°ì´í„°ì…‹)ë¥¼ ê²°í•©í•˜ê¸°ë¡œ ê²°ì •í–ˆë‹¤.


| ê°ì •   | ê°œìˆ˜   |
|--------|-------:|
| í–‰ë³µ   | 1,030  |
| ì¤‘ë¦½   | 43,786 |
| ìŠ¬í””   | 1,972  |
| ê³µí¬   | 98     |
| í˜ì˜¤   | 220    |
| ë¶„ë…¸   | 3,628  |
| ë†€ëŒ   | 4,866  |

### 1) ì—°ì†ì  ëŒ€í™” ë°ì´í„°ì…‹ ì „ì²˜ë¦¬

- ì¶•ì•½/ì˜¤íƒ€ ë ˆì´ë¸” ì •ë¦¬

```python
data3.replace('ã…', 'ê³µí¬', inplace=True)
data3.replace(['ë¶„', 'ë¶„ã„´'], 'ë¶„ë…¸', inplace=True)
data3.replace(['ã…ˆì¤‘ë¦½', 'ì¤‘ë¦¼', 'ã„´ì¤‘ë¦½', 'ì¤„'], 'ë¶„ë…¸', inplace=True)
```

- (Sentence, Emotion) í˜•íƒœ ì •ë¦¬

```python
data3.rename(columns={'Unnamed: 1': 'Sentence', 'Unnamed: 2': 'Emotion'}, inplace=True)
```

### 2) ê°ì„± ëŒ€í™” ë§ë­‰ì¹˜ ì „ì²˜ë¦¬

- ë¶ˆí•„ìš”í•œ ì»¬ëŸ¼ ì œê±°

```python
corpus.drop(
    ['Unnamed: 0', 'ì—°ë ¹', 'ì„±ë³„', 'ì‹ ì²´ì§ˆí™˜',
     'ì‹œìŠ¤í…œë¬¸ì¥1', 'ì‹œìŠ¤í…œë¬¸ì¥2', 'ì‹œìŠ¤í…œë¬¸ì¥3'],
    axis=1,
    inplace=True
)
```

- ê°ì •_ì†Œë¶„ë¥˜ â†’ 5ê°œ ê°ì • ë§¤í•‘
  - **í–‰ë³µ**

    `ë§Œì¡±ìŠ¤ëŸ¬ìš´`, `í¸ì•ˆí•œ`, `ì‹ ë¢°í•˜ëŠ”`, `ì•ˆë„`, `ê¸°ì¨`, `ê°ì‚¬í•˜ëŠ”`, `ì‹ ì´ ë‚œ`, `ìì‹ í•˜ëŠ”`, `ëŠê¸‹` ë“±
  - **ê³µí¬**

    `í˜¼ë€ìŠ¤ëŸ¬ìš´`, `ë‘ë ¤ìš´`, `ë¶ˆì•ˆ`, `ì´ˆì¡°í•œ`, `ë‹¹í˜¹ìŠ¤ëŸ¬ìš´` ë“±
  - **ìŠ¬í””**

    `ì¢Œì ˆí•œ`, `ëˆˆë¬¼ì´ ë‚˜ëŠ”`, `ìš°ìš¸í•œ`, `ìŠ¬í””`, `ìƒì²˜`, `ì‹¤ë§í•œ`, `í›„íšŒë˜ëŠ”`, `ë¹„í†µí•œ`, `ë‚™ë‹´í•œ`, `ë²„ë ¤ì§„`, `í¬ìƒëœ`, `ê³ ë¦½ëœ` ë“±
  - **ë¶„ë…¸**

    `ì§œì¦ë‚´ëŠ”`, `ë¶„ë…¸`, `êµ¬ì—­ì§ˆ ë‚˜ëŠ”`, `í™˜ë©¸ì„ ëŠë¼ëŠ”`, `ì•…ì˜ì ì¸`, `í˜ì˜¤ìŠ¤ëŸ¬ìš´` ë“±
  - **ì¤‘ë¦½**

    ì½”ë“œ ìƒì—ì„œ í•„ìš” ì—†ëŠ”/ì• ë§¤í•œ ì†Œë¶„ë¥˜ë¥¼ ì œê±°í•˜ê³ , ë‚˜ë¨¸ì§€ëŠ” ì¤‘ë¦½ ë¬¸ì¥ìœ¼ë¡œ ì·¨ê¸‰

- (Sentence, Emotion) í˜•íƒœ ì •ë¦¬

```python
corpus.drop(['ìƒí™©í‚¤ì›Œë“œ','ê°ì •_ì†Œë¶„ë¥˜','ì‚¬ëŒë¬¸ì¥2', 'ì‚¬ëŒë¬¸ì¥3'], axis=1, inplace=True)

corpus.rename(columns={'ê°ì •_ëŒ€ë¶„ë¥˜': 'Emotion', 'ì‚¬ëŒë¬¸ì¥1': 'Sentence'}, inplace=True)

corpus = corpus[['Sentence', 'Emotion']]
corpus = corpus.reset_index(drop=True)
```

### 3) í˜ì˜¤ í‘œí˜„ ë°ì´í„°ì…‹ ì „ì²˜ë¦¬

- í˜ì˜¤ ë¬¸ì¥ë§Œ í•„í„°ë§

```python
data4 = data4[data4["í˜ì˜¤"] == 1]
```

- (Sentence, Emotion) í˜•íƒœ ì •ë¦¬

```python
data4 = data4[['ë¬¸ì¥', 'í˜ì˜¤']]
data4.rename(columns={'ë¬¸ì¥': 'Sentence', 'í˜ì˜¤': 'Emotion'}, inplace=True)
data4['Emotion'].loc[data4["Emotion"] == 1] = 'í˜ì˜¤'
```

### 4) ë°ì´í„° ê²°í•©

ìœ„ì˜ ë°ì´í„°ë“¤ì„ ê²°í•©í•˜ì—¬ ìµœì¢…ì ìœ¼ë¡œ í´ë˜ìŠ¤ ë¶ˆê· í˜•ì„ ì™„í™”í•œ í†µí•© ë°ì´í„°í”„ë ˆì„ `data`ë¥¼ ì™„ì„±í–ˆë‹¤.

| ê°ì •   | ê°œìˆ˜   |
|--------|-------:|
| í–‰ë³µ   | 7,725  |
| ì¤‘ë¦½   | 40,813 |
| ìŠ¬í””   | 15,144 |
| ê³µí¬   | 6,226  |
| í˜ì˜¤   | 11,457 |
| ë¶„ë…¸   | 9,790  |
| ë†€ëŒ   | 3,979  |

## 2. ëª¨ë¸ í•™ìŠµ

### 1) KoBERT ê¸°ë°˜ ê°ì • ë¶„ë¥˜ ëª¨ë¸
ì „ì²˜ë¦¬ëœ í†µí•© ë°ì´í„°ì…‹(data.csv)ì„ ë°”íƒ•ìœ¼ë¡œ, ë¨¼ì € KoBERT ê¸°ë°˜ í•œêµ­ì–´ ê°ì • ë¶„ë¥˜ ëª¨ë¸ì„ ì „í˜•ì ì¸ íŒŒì¸íŠœë‹ êµ¬ì¡°ë¥¼ í†µí•´ í•™ìŠµí–ˆë‹¤. SKTBrainì—ì„œ ê³µê°œí•œ KoBERTëŠ” í•œêµ­ì–´ ìœ„í‚¤ ë¬¸ì¥ ì•½ 500ë§Œ ê°œì™€ í•œêµ­ì–´ ë‰´ìŠ¤ ë¬¸ì¥ ì•½ 2,000ë§Œ ê°œë¥¼ í•™ìŠµí•œ ëª¨ë¸ë¡œ, ì¼ë°˜ BERTì— ë¹„í•´ í•œêµ­ì–´ ë¬¸ì¥ ì´í•´ ì„±ëŠ¥ì´ ìš°ìˆ˜í•˜ë©°, ë‹¨ìˆœ ê¸Â·ë¶€ì • ì´ì§„ ë¶„ë¥˜ê°€ ì•„ë‹ˆë¼ ì—¬ëŸ¬ ê°ì •ìœ¼ë¡œì˜ ë‹¤ì¤‘ ë¶„ë¥˜ì—ë„ ì í•©í•˜ë‹¤ê³  í‰ê°€ëœë‹¤. ë³¸ í”„ë¡œì íŠ¸ì—ì„œëŠ” HuggingFace Hubì— ê³µê°œëœ skt/kobert-base-v1 ì²´í¬í¬ì¸íŠ¸ë¥¼ ì‚¬ìš©í•˜ë©°, ì˜ì¡´ì„± ë¬¸ì œë¥¼ í•´ê²°í•˜ê³  ìµœì‹  í™˜ê²½ì—ì„œë„ ì•ˆì •ì ìœ¼ë¡œ ë™ì‘í•˜ë„ë¡ í•˜ê¸° ìœ„í•´ ê¸°ì¡´ MXNet/GluonNLP ê¸°ë°˜ êµ¬í˜„ì„ HuggingFace ê¸°ë°˜ í† í¬ë‚˜ì´ì €ë¡œ ëŒ€ì²´í•˜ê³  vocab êµ¬ì¡°ë¥¼ ì§ì ‘ ë°›ì•„ì˜¤ëŠ” BertModel + PyTorch êµ¬ì¡°ë¡œ ì¬êµ¬ì„±í–ˆë‹¤.
- KoBERT
(https://github.com/SKTBrain/KoBERT)

```sh
pip install 'git+https://github.com/SKTBrain/KoBERT.git#egg=kobert_tokenizer&subdirectory=kobert_hf'
```

- í•™ìŠµì„ ì§„í–‰í•˜ê¸° ìœ„í•´ ê°ì • ë ˆì´ë¸”ì„ 0~6ì˜ ì •ìˆ˜ë¡œ ë§¤í•‘í•˜ê³  train_test_splitì„ ì´ìš©í•´ í•™ìŠµ/ê²€ì¦ ë°ì´í„°ë¥¼ 8:2 ë¹„ìœ¨ë¡œ ë‚˜ëˆ„ì—ˆë‹¤. ì´ë•Œ ê° ê°ì • í´ë˜ìŠ¤ì˜ ë¶„í¬ê°€ í•™ìŠµ/ê²€ì¦ ì…‹ì—ì„œ ë¹„ìŠ·í•˜ê²Œ ìœ ì§€ë˜ë„ë¡ stratify ì˜µì…˜ì„ ì‚¬ìš©í–ˆë‹¤.
```python
target_classes = {
    'ê³µí¬': 0,
    'ë†€ëŒ': 1,
    'ë¶„ë…¸': 2,
    'ìŠ¬í””': 3,
    'ì¤‘ë¦½': 4,
    'í–‰ë³µ': 5,
    'í˜ì˜¤': 6
}
id2label = {v: k for k, v in target_classes.items()}
num_labels = len(target_classes)

if df['emotion'].dtype == object:
    df['emotion'] = df['emotion'].map(target_classes).astype(int)
```

```python
train_df, valid_df = train_test_split(
    df,
    test_size=0.2,
    random_state=42,
    stratify=df['emotion']
)
```

- KoBERTëŠ” ë‹¨ìˆœ ë¬¸ìì—´ì´ ì•„ë‹ˆë¼ input_ids, segment_ids, valid_length í˜•íƒœì˜ í…ì„œë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ê¸° ë•Œë¬¸ì—, ì´ë¥¼ ìë™ìœ¼ë¡œ ë§Œë“¤ì–´ ì£¼ëŠ” BERTSentenceTransform í´ë˜ìŠ¤ë¥¼ êµ¬í˜„í•˜ì˜€ë‹¤. BERTSentenceTransform í´ë˜ìŠ¤ëŠ” í•˜ë‚˜ì˜ ë¬¸ì¥ì„ í† í¬ë‚˜ì´ì €ë¡œ í† í°í™”í•œ ë’¤, [CLS]ì™€ [SEP] í† í°ì„ ë¶™ì´ê³ , ìµœëŒ€ ê¸¸ì´(max_seq_length)ë¥¼ ë„˜ëŠ” ë¶€ë¶„ì„ ì˜ë¼ë‚¸ ë‹¤ìŒ, ë¶€ì¡±í•œ ë¶€ë¶„ì€ PAD í† í°ìœ¼ë¡œ ì±„ì›Œì„œ ê³ ì • ê¸¸ì´ ì‹œí€€ìŠ¤ë¡œ ë§Œë“œëŠ” ì—­í• ì„ í•œë‹¤. ì¦‰, í•œ ë¬¸ì¥ì„ KoBERTê°€ ë°”ë¡œ ë°›ì„ ìˆ˜ ìˆëŠ” í† í° ID ì‹œí€€ìŠ¤ë¡œ ë³€í™˜í•´ì£¼ëŠ” ì—­í• ì„ í•œë‹¤.
```python
class BERTSentenceTransform:
    def __init__(self, tokenizer, max_seq_length,
                 pad=True, pair=False):
        self._tokenizer = tokenizer
        self._max_seq_length = max_seq_length
        self._pad = pad
        self._pair = pair

        self.cls_token = self._tokenizer.cls_token or '[CLS]'
        self.sep_token = self._tokenizer.sep_token or '[SEP]'
        self.pad_token_id = self._tokenizer.pad_token_id

    def _truncate_seq_pair(self, tokens_a, tokens_b, max_length):
        while True:
            total_length = len(tokens_a) + len(tokens_b)
            if total_length <= max_length:
                break
            if len(tokens_a) > len(tokens_b):
                tokens_a.pop()
            else:
                tokens_b.pop()

    def __call__(self, line):
        text_a = line[0]
        tokens_a = self._tokenizer.tokenize(text_a)
        tokens_b = None

        if self._pair:
            assert len(line) == 2
            text_b = line[1]
            tokens_b = self._tokenizer.tokenize(text_b)

        if tokens_b:
            self._truncate_seq_pair(tokens_a, tokens_b,
                                    self._max_seq_length - 3)
        else:
            if len(tokens_a) > self._max_seq_length - 2:
                tokens_a = tokens_a[:self._max_seq_length - 2]

        tokens = []
        tokens.append(self.cls_token)
        tokens.extend(tokens_a)
        tokens.append(self.sep_token)
        segment_ids = [0] * len(tokens)

        if tokens_b:
            tokens.extend(tokens_b)
            tokens.append(self.sep_token)
            segment_ids.extend([1] * (len(tokens) - len(segment_ids)))

        input_ids = self._tokenizer.convert_tokens_to_ids(tokens)
        valid_length = len(input_ids)

        if self._pad:
            padding_length = self._max_seq_length - valid_length
            input_ids.extend([self.pad_token_id] * padding_length)
            segment_ids.extend([0] * padding_length)

        return (np.array(input_ids, dtype='int32'),
                np.array(valid_length, dtype='int32'),
                np.array(segment_ids, dtype='int32'))
```

- BERTDataset í´ë˜ìŠ¤ëŠ” í•™ìŠµìš© ë°ì´í„° êµ¬ì¡°ë¥¼ PyTorch DataLoaderì— ë°”ë¡œ ì…ë ¥ ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë§Œë“¤ê¸° ìœ„í•´ [sentence, label] êµ¬ì¡°ì˜ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°›ì•„, ê° ë¬¸ì¥ì„ BERTSentenceTransformìœ¼ë¡œ ë³€í™˜í•˜ê³  PyTorch Dataset í˜•íƒœë¡œ ê°ì‹¸ëŠ” ì—­í• ì„ í•œë‹¤.
```python
class BERTDataset(Dataset):
    def __init__(self, dataset, sent_idx, label_idx,
                 bert_tokenizer, max_len,
                 pad=True, pair=False):

        transform = BERTSentenceTransform(
            bert_tokenizer,
            max_seq_length=max_len,
            pad=pad,
            pair=pair
        )
        self.sentences = [transform([i[sent_idx]]) for i in dataset]
        self.labels = [np.int32(i[label_idx]) for i in dataset]

    def __getitem__(self, i):
        return self.sentences[i] + (self.labels[i], )

    def __len__(self):
        return len(self.labels)
```

- ì‹¤ì œ ë°ì´í„°ë¡œë¶€í„° KoBERT í† í¬ë‚˜ì´ì €ì™€ ì‚¬ì „í•™ìŠµëœ BertModelì„ ë¶ˆëŸ¬ì˜¨ ë’¤, ì „ì²˜ë¦¬ëœ ë°ì´í„°í”„ë ˆì„ì„ ê¸°ë°˜ìœ¼ë¡œ ë°°ì¹˜ ë‹¨ìœ„ í•™ìŠµ/ê²€ì¦ìš© DataLoaderë¥¼ êµ¬ì„±í•˜ëŠ” ì½”ë“œëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.
```python
# KoBERT í† í¬ë‚˜ì´ì € & ëª¨ë¸
tokenizer_kobert = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1')
bertmodel_kobert = BertModel.from_pretrained('skt/kobert-base-v1')

data_list_train = [
    [q, str(l)] for q, l in zip(train_df['sentence'], train_df['emotion'])
]
data_list_valid = [
    [q, str(l)] for q, l in zip(valid_df['sentence'], valid_df['emotion'])
]

data_train = BERTDataset(data_list_train, 0, 1, tokenizer_kobert, max_len_kobert, pad=True, pair=False)
data_valid = BERTDataset(data_list_valid, 0, 1, tokenizer_kobert, max_len_kobert, pad=True, pair=False)

train_dataloader_kobert = DataLoader(
    data_train,
    batch_size=batch_size_kobert,
    num_workers=0,
    shuffle=True
)
valid_dataloader_kobert = DataLoader(
    data_valid,
    batch_size=batch_size_kobert,
    num_workers=0,
    shuffle=False
)
```

- KoBERT ê¸°ë°˜ ê°ì • ë¶„ë¥˜ ëª¨ë¸ì€ ì‚¬ì „í•™ìŠµëœ BertModel ìœ„ì— ë‹¨ì¼ ì„ í˜• ë¶„ë¥˜ ë ˆì´ì–´ë¥¼ ì˜¬ë ¤ 7ê°œ ê°ì • í´ë˜ìŠ¤ë¥¼ ì˜ˆì¸¡í•˜ëŠ” êµ¬ì¡°ë¡œ ì„¤ê³„í–ˆë‹¤. ì‚¬ì „í•™ìŠµëœ KoBERTì—ì„œ CLS ìœ„ì¹˜ì˜ pooled_outputì„ ê°€ì ¸ì™€ Dropoutìœ¼ë¡œ ê³¼ì í•©ì„ ì™„í™”í•˜ê³  ì •ê·œí™”í•œ ë’¤ ìµœì¢… 7ì°¨ì›ìœ¼ë¡œ íˆ¬ì˜í•˜ëŠ” ê³¼ì •ì„ ì •ì˜í•˜ë©°, softmaxë¥¼ ì ìš©í•´ ê° ê°ì • í´ë˜ìŠ¤ì— ëŒ€í•œ í™•ë¥ ì„ ì–»ë„ë¡ í•˜ëŠ” ì—­í• ì„ í•œë‹¤.
```python
class BERTClassifier(nn.Module):
    def __init__(self,
                 bert,
                 hidden_size=768,
                 num_classes=7,
                 dr_rate=None):
        super(BERTClassifier, self).__init__()
        self.bert = bert
        self.dr_rate = dr_rate

        self.classifier = nn.Linear(hidden_size, num_classes)
        if dr_rate:
            self.dropout = nn.Dropout(p=dr_rate)

    def gen_attention_mask(self, token_ids, valid_length):
        attention_mask = torch.zeros_like(token_ids)
        for i, v in enumerate(valid_length):
            attention_mask[i][:v] = 1
        return attention_mask.float()

    def forward(self, token_ids, valid_length, segment_ids):
        attention_mask = self.gen_attention_mask(token_ids, valid_length)
        _, pooler = self.bert(
            input_ids=token_ids,
            token_type_ids=segment_ids.long(),
            attention_mask=attention_mask.to(token_ids.device),
            return_dict=False
        )
        if self.dr_rate:
            out = self.dropout(pooler)
        else:
            out = pooler
        return self.classifier(out)
```

- í•™ìŠµì— ì‚¬ìš©í•œ ì£¼ìš” í•˜ì´í¼íŒŒë¼ë¯¸í„°ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.
```python
max_len_kobert = 100
batch_size_kobert = 64
warmup_ratio = 0.1
num_epochs_kobert = 1
max_grad_norm = 1
log_interval = 200
learning_rate_kobert = 5e-5

kobert_model = BERTClassifier(bertmodel_kobert, dr_rate=0.5).to(device_kobert)
```

- ì˜µí‹°ë§ˆì´ì €ëŠ” BERT ê³„ì—´ íŒŒì¸íŠœë‹ì—ì„œ í‘œì¤€ì ìœ¼ë¡œ ì‚¬ìš©í•˜ëŠ” AdamWë¥¼ ì‚¬ìš©í•˜ê³ , AdamW + weight decay + cosine LR scheduler + warmup ì¡°í•©ì„ í†µí•´ í•™ìŠµ ì´ˆê¸°ì—ëŠ” í•™ìŠµë¥ ì„ ì ì§„ì ìœ¼ë¡œ ì¦ê°€ì‹œí‚¤ê³ , ì´í›„ì—ëŠ” ì ì°¨ ê°ì†Œì‹œí‚¤ë©´ì„œ ì•ˆì •ì ì¸ íŒŒì¸íŠœë‹ì„ ìœ ë„í•˜ë„ë¡ ì„¤ê³„í•˜ì˜€ë‹¤.
```python
no_decay = ['bias', 'LayerNorm.weight']
optimizer_grouped_parameters = [
    {
        'params': [p for n, p in kobert_model.named_parameters()
                   if not any(nd in n for nd in no_decay)],
        'weight_decay': 0.01
    },
    {
        'params': [p for n, p in kobert_model.named_parameters()
                   if any(nd in n for nd in no_decay)],
        'weight_decay': 0.0
    }
]

optimizer_kobert = AdamW(optimizer_grouped_parameters, lr=learning_rate_kobert)
loss_fn_kobert = nn.CrossEntropyLoss()

t_total = len(train_dataloader_kobert) * num_epochs_kobert
warmup_step = int(t_total * warmup_ratio)

scheduler_kobert = get_cosine_schedule_with_warmup(
    optimizer_kobert,
    num_warmup_steps=warmup_step,
    num_training_steps=t_total
)
```

- í•™ìŠµ ë£¨í”„ëŠ” ì—í­ ë‹¨ìœ„ë¡œ ë°˜ë³µë˜ë©°, ê° ì—í­ì—ì„œ í•™ìŠµ ë‹¨ê³„ì™€ ê²€ì¦ ë‹¨ê³„ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ìˆ˜í–‰í•œë‹¤. ê° ì—í­ì´ ëë‚œ ë’¤ì—ëŠ” eval() ëª¨ë“œì—ì„œ ê²€ì¦ ë°ì´í„°ë¥¼ í•œ ë²ˆ ë” í†µê³¼ì‹œì¼œ validation accuracyë¥¼ ê³„ì‚°í•˜ê³ , train_historyì™€ valid_historyì— ê¸°ë¡í•˜ì—¬ ì´í›„ í•™ìŠµ ê³¡ì„ ì„ ì‹œê°í™”í•  ìˆ˜ ìˆë‹¤.
```python
train_history = []
valid_history = []
loss_history = []

for epoch in range(num_epochs_kobert):
    kobert_model.train()
    train_acc = 0.0
    batch_count = 0
    print(f"\n===== KoBERT Epoch {epoch+1} / {num_epochs_kobert} =====")

    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm(train_dataloader_kobert)):
        optimizer_kobert.zero_grad()

        token_ids = token_ids.long().to(device_kobert)
        segment_ids = segment_ids.long().to(device_kobert)
        valid_length = valid_length
        label = label.long().to(device_kobert)

        logits = kobert_model(token_ids, valid_length, segment_ids)
        loss = loss_fn_kobert(logits, label)

        loss.backward()
        nn.utils.clip_grad_norm_(kobert_model.parameters(), max_grad_norm)
        optimizer_kobert.step()
        scheduler_kobert.step()

        batch_acc = calc_accuracy(logits, label)
        train_acc += batch_acc
        batch_count += 1

        if batch_id % log_interval == 0:
            print(f"Epoch {epoch+1} Batch {batch_id+1} "
                  f"Loss {loss.item():.4f} Train Acc {train_acc / batch_count:.4f}")

        train_history.append(train_acc / batch_count)
        loss_history.append(loss.item())

    print(f"Epoch {epoch+1} Train Acc: {train_acc / batch_count:.4f}")

    # ----- ê²€ì¦ -----
    kobert_model.eval()
    valid_acc = 0.0
    valid_batch_count = 0
    with torch.no_grad():
        for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm(valid_dataloader_kobert)):
            token_ids = token_ids.long().to(device_kobert)
            segment_ids = segment_ids.long().to(device_kobert)
            valid_length = valid_length
            label = label.long().to(device_kobert)

            logits = kobert_model(token_ids, valid_length, segment_ids)
            batch_acc = calc_accuracy(logits, label)
            valid_acc += batch_acc
            valid_batch_count += 1

    print(f"Epoch {epoch+1} Valid Acc: {valid_acc / valid_batch_count:.4f}")
    valid_history.append(valid_acc / valid_batch_count)
```

- í•™ìŠµì´ ëë‚˜ë©´, ì¶”í›„ Streamlit ì•±ì—ì„œ ì¬ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ê°€ì¤‘ì¹˜ë¥¼ ì €ì¥í•œë‹¤. ì´ë ‡ê²Œ ì €ì¥ëœ .pt íŒŒì¼ì€ app.pyì—ì„œ ë¡œë“œí•˜ì—¬, ì‹¤ì œ ì‚¬ìš©ì ì…ë ¥ì— ëŒ€í•œ ì‹¤ì‹œê°„ ê°ì • ë¶„ì„ ë° GIF ì¶”ì²œì— í™œìš©ëœë‹¤.
```python
torch.save(kobert_model.state_dict(), "./kobert_emotion_model_state_dict.pt")
print("ëª¨ë¸ ê°€ì¤‘ì¹˜ ì €ì¥ ì™„ë£Œ: kobert_emotion_model_state_dict.pt")
```

### 2) KoELECTRA ê¸°ë°˜ ê°ì • ë¶„ë¥˜ ëª¨ë¸
ë³¸ í”„ë¡œì íŠ¸ì—ì„œëŠ” KoBERTì™€ ë”ë¶ˆì–´, í•œêµ­ì–´ì— íŠ¹í™”ëœ ì‚¬ì „í•™ìŠµ ì–¸ì–´ëª¨ë¸ì¸ KoELECTRA ë˜í•œ í•¨ê»˜ ì‹¤í—˜í–ˆë‹¤. KoELECTRAëŠ” ELECTRA êµ¬ì¡°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ í•œêµ­ì–´ ëª¨ë¸ë¡œ BERT ëŒ€ë¹„ í•™ìŠµ íš¨ìœ¨ì´ ë†’ê³ , ë‹¤ì–‘í•œ í•œêµ­ì–´ ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ íƒœìŠ¤í¬ì—ì„œ ê°•í•œ ì„±ëŠ¥ì„ ë³´ì´ëŠ” ê²ƒìœ¼ë¡œ ì•Œë ¤ì ¸ ìˆë‹¤. ë”°ë¼ì„œ ë™ì¼í•œ ê°ì • ë¶„ë¥˜ ë°ì´í„°ì…‹ì— ëŒ€í•´ KoBERTì™€ KoELECTRAë¥¼ ê°ê° íŒŒì¸íŠœë‹í•˜ê³ , ë‘ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ë¹„êµí•˜ëŠ” ì‹¤í—˜ì„ ì§„í–‰í–ˆë‹¤.

- KoELECTRA (https://github.com/monologg/KoELECTRA)

- KoELECTRAì™€ ê°™ì€ HuggingFace ê¸°ë°˜ ëª¨ë¸ì´ í•™ìŠµí•  ìˆ˜ ìˆë„ë¡, ë¬¸ì¥ê³¼ ê°ì • ë ˆì´ë¸”ì„ ëª¨ë¸ ì…ë ¥ í¬ë§·ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” EmotionDataset í´ë˜ìŠ¤ë¥¼ ì •ì˜í–ˆë‹¤. HuggingFace ëª¨ë¸ì€ ë‹¨ìˆœ ë¬¸ìì—´ì„ ë°”ë¡œ ì…ë ¥ë°›ì§€ ëª»í•˜ê³ , ë°˜ë“œì‹œ input_ids, attention_mask, token_type_ids í˜•íƒœì˜ í…ì„œë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ì•¼ í•œë‹¤. ë”°ë¼ì„œ ì´ ë°ì´í„°ì…‹ í´ë˜ìŠ¤ëŠ” í† í¬ë‚˜ì´ì €ë¥¼ ì´ìš©í•´ ê° ë¬¸ì¥ì„ í•´ë‹¹ êµ¬ì¡°ë¡œ ìë™ ë³€í™˜í•˜ê³ , ë ˆì´ë¸”ë„ PyTorch í…ì„œë¡œ í•¨ê»˜ ë°˜í™˜í•œë‹¤.
```python
class EmotionDataset(Dataset):
    def __init__(self, sentences, labels, tokenizer, max_len=64):
        self.sentences = sentences.reset_index(drop=True)
        self.labels = labels.reset_index(drop=True)
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self):
        return len(self.sentences)

    def __getitem__(self, idx):
        sentence = str(self.sentences.iloc[idx])
        label = int(self.labels.iloc[idx])

        encoded = self.tokenizer(
            sentence,
            padding='max_length',
            truncation=True,
            max_length=self.max_len,
            return_tensors='pt'
        )

        item = {k: v.squeeze(0) for k, v in encoded.items()}
        item['labels'] = torch.tensor(label, dtype=torch.long)
        return item
```

- evaluate_hf() í•¨ìˆ˜ëŠ” í•™ìŠµëœ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‰ê°€í•˜ê¸° ìœ„í•œ ìœ í‹¸ë¦¬í‹°ë¡œ, ì£¼ì–´ì§„ dataloaderë¥¼ ìˆœíšŒí•˜ë©° ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œê°’ì„ ë¹„êµí•˜ì—¬ accuracyì™€ macro F1-scoreë¥¼ ê³„ì‚°í•œë‹¤.
```python
def evaluate_hf(model, dataloader, device):
    model.eval()
    preds, trues = [], []

    with torch.no_grad():
        for batch in dataloader:
            batch = {k: v.to(device) for k, v in batch.items()}
            outputs = model(**batch)
            logits = outputs.logits
            pred = torch.argmax(logits, dim=-1)

            preds.extend(pred.cpu().tolist())
            trues.extend(batch['labels'].cpu().tolist())

    acc = accuracy_score(trues, preds)
    macro_f1 = f1_score(trues, preds, average='macro')
    return acc, macro_f1, np.array(trues), np.array(preds)
```

- train_one_model()ì€ HuggingFace ëª¨ë¸ì„ ì‹¤ì œë¡œ í•™ìŠµì‹œí‚¤ëŠ” í•¨ìˆ˜ë¡œ, í† í¬ë‚˜ì´ì € ë¡œë“œ â†’ ë°ì´í„°ì…‹/ë°ì´í„°ë¡œë” êµ¬ì„± â†’ í•™ìŠµ ë£¨í”„ ì‹¤í–‰ â†’ validation í‰ê°€ ê³¼ì •ìœ¼ë¡œ ì´ë£¨ì–´ì§„ë‹¤. ì´ í•¨ìˆ˜ëŠ” ìµœì¢…ì ìœ¼ë¡œ í•™ìŠµëœ ëª¨ë¸, í† í¬ë‚˜ì´ì €, best accuracy, best F1-scoreë¥¼ ë°˜í™˜í•œë‹¤. ì´ êµ¬ì„±ì€ ì´í›„ KoELECTRA í•™ìŠµÂ·í‰ê°€ ë‹¨ê³„ì˜ ê¸°ë°˜ ì—­í• ì„ í•œë‹¤.
```python
def train_one_model(model_name, train_df, valid_df, num_labels, alias=None, epochs=1, batch_size=32, lr=2e-5, max_len=64, device_override=None):
    if alias is None:
        alias = model_name
    print(f"\n=== ëª¨ë¸: {alias} ({model_name}) ===")

    if device_override is not None:
        dev = torch.device(device_override)
    else:
        dev = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print("ì‚¬ìš© device:", dev)

    assert train_df['emotion'].min() >= 0 and train_df['emotion'].max() < num_labels
    assert valid_df['emotion'].min() >= 0 and valid_df['emotion'].max() < num_labels

    # í† í¬ë‚˜ì´ì € & ëª¨ë¸ ë¡œë“œ
    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)
    model = AutoModelForSequenceClassification.from_pretrained(
        model_name,
        num_labels=num_labels
    )

    model.resize_token_embeddings(len(tokenizer))
    model.to(dev)

    print("tokenizer vocab_size:", len(tokenizer))
    print("input embedding size:", model.get_input_embeddings().weight.shape)

    train_dataset = EmotionDataset(
        train_df['sentence'],
        train_df['emotion'],
        tokenizer,
        max_len=max_len
    )
    valid_dataset = EmotionDataset(
        valid_df['sentence'],
        valid_df['emotion'],
        tokenizer,
        max_len=max_len
    )

    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)

    optimizer = AdamW(model.parameters(), lr=lr)

    best_acc = 0.0
    best_f1 = 0.0

    for epoch in range(1, epochs + 1):
        model.train()
        total_loss = 0.0
        loop = tqdm(train_loader, desc=f"Epoch {epoch}")

        for batch in loop:
            batch = {k: v.to(dev) for k, v in batch.items()}

            outputs = model(**batch)
            loss = outputs.loss

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            total_loss += loss.item()
            loop.set_postfix(loss=loss.item())

        val_acc, val_f1, _, _ = evaluate_hf(model, valid_loader, dev)
        avg_loss = total_loss / len(train_loader)
        print(f"[Epoch {epoch}] loss: {avg_loss:.4f}, val_acc: {val_acc:.4f}, val_macro_f1={val_f1:.4f}")

        if val_acc > best_acc:
            best_acc = val_acc
            best_f1 = val_f1

    print(f"=== {alias} ìµœì¢… ì„±ëŠ¥: acc={best_acc:.4f}, macro_f1={best_f1:.4f} ===")
    return best_acc, best_f1, model, tokenizer
```

- ì‚¬ì „í•™ìŠµëœ KoELECTRA-base-v3-discriminator ëª¨ë¸ì„ ì‚¬ìš©í•´ ê°ì • ë¶„ë¥˜ ì‘ì—…ì— ë§ê²Œ íŒŒì¸íŠœë‹ì„ ìˆ˜í–‰í•œë‹¤. train_one_model() í•¨ìˆ˜ëŠ” ì£¼ì–´ì§„ í•™ìŠµ ë°ì´í„°(train_df)ì™€ ê²€ì¦ ë°ì´í„°(valid_df)ë¥¼ ì…ë ¥ë°›ì•„ ëª¨ë¸ì„ í•œ epoch ë™ì•ˆ í•™ìŠµì‹œí‚¤ê³ , ìµœì¢…ì ìœ¼ë¡œ Validation Accuracyì™€ Macro F1-score, ê·¸ë¦¬ê³  í•™ìŠµì´ ì™„ë£Œëœ ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ê°ì²´ë¥¼ ë°˜í™˜í•œë‹¤. í•™ìŠµì´ ì™„ë£Œë˜ë©´, ëª¨ë¸ì€(validation ê¸°ì¤€) ê°ì • ë¶„ë¥˜ ì •í™•ë„ì™€ F1-scoreê°€ ì¶œë ¥ë˜ë©°, ì´í›„ ì‹¤ì‚¬ìš©ì„ ìœ„í•œ ì˜ˆì¸¡ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¬¸ì¥ ê°ì • ë¶„ì„ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆë‹¤.
```python
acc_koel, f1_koel, model_koel, tokenizer_koel = train_one_model(
    model_name="monologg/koelectra-base-v3-discriminator",
    alias="KoELECTRA",
    train_df=train_df,
    valid_df=valid_df,
    num_labels=num_labels,
    epochs=1,
    batch_size=32,
    lr=2e-5,
    max_len=64
)

print("KoELECTRA ì„±ëŠ¥ â†’ acc:", acc_koel, "macro_f1:", f1_koel)
```

- ê¸°ë³¸ ì˜ˆì¸¡ í•¨ìˆ˜ì¸ predict_emotion_hf()ëŠ” ì…ë ¥ ë¬¸ì¥ì„ í† í¬ë‚˜ì´ì €ë¡œ ì¸ì½”ë”©í•œ ë’¤, KoELECTRA ëª¨ë¸ì— ì „ë‹¬í•˜ì—¬ ê°ì •ë³„ ë¡œì§“(logit)ì„ ê³„ì‚°í•œë‹¤. KoELECTRAì—ì„œë„ ëª¨ë¸ ì¶œë ¥ê°’ì€ softmaxë¥¼ í†µí•´ í™•ë¥ ë¡œ ë³€í™˜ë˜ë©°, ê°€ì¥ ë†’ì€ í™•ë¥ ì„ ê°€ì§„ ê°ì • í´ë˜ìŠ¤ê°€ ìµœì¢… ì˜ˆì¸¡ ê²°ê³¼ë¡œ ì„ íƒëœë‹¤.
```python
def predict_emotion_hf(sentence, model, tokenizer, id2label, max_len=64):
    model.eval()
    dev = next(model.parameters()).device

    encoded = tokenizer(
        sentence,
        return_tensors='pt',
        truncation=True,
        padding='max_length',
        max_length=max_len
    )
    encoded = {k: v.to(dev) for k, v in encoded.items()}

    with torch.no_grad():
        outputs = model(**encoded)
        logits = outputs.logits
        probs = torch.softmax(logits, dim=-1)[0]
        pred_id = int(torch.argmax(probs))

    pred_label = id2label[pred_id]
    return pred_id, pred_label, probs.cpu().numpy()
```

- ë¬¸ì¥ì„ ì…ë ¥í•˜ë©´ ëª¨ë¸ì— ì „ë‹¬ë˜ì–´ ê°ì •ì„ ì˜ˆì¸¡í•˜ê³ , ì˜ˆì¸¡ëœ ê°ì • ë ˆì´ë¸”ê³¼ ê°ì •ë³„ í™•ë¥ ì„ í•¨ê»˜ ì¶œë ¥í•œë‹¤. ì¶œë ¥ëœ ê²°ê³¼ë¥¼ í†µí•´ ëª¨ë¸ì´ íŠ¹ì • ë¬¸ì¥ì„ ì–´ë–»ê²Œ í•´ì„í•˜ëŠ”ì§€ ì§ê´€ì ìœ¼ë¡œ í™•ì¸í•  ìˆ˜ ìˆë‹¤.
```python
test_sent = "ì˜¤ëŠ˜ ì €ë…ì€ ì¹˜í‚¨ì´ë‹­"
pred_id, pred_label, probs = predict_emotion_hf(
    test_sent,
    model_koel,
    tokenizer_koel,
    id2label
)

print("ì…ë ¥ ë¬¸ì¥:", test_sent)
print("ì˜ˆì¸¡ ê°ì • ì½”ë“œ:", pred_id)
print("ì˜ˆì¸¡ ê°ì • ì´ë¦„:", pred_label)
print("ê° ê°ì •ë³„ í™•ë¥ :", probs)
```

- í•™ìŠµì´ ëë‚˜ë©´, ì¶”í›„ Streamlit ì•±ì—ì„œ ì¬ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ê°€ì¤‘ì¹˜ë¥¼ ì €ì¥í•œë‹¤. ì´ë ‡ê²Œ ì €ì¥ëœ .pt íŒŒì¼ì€ app.pyì—ì„œ ë¡œë“œí•˜ì—¬, ì‹¤ì œ ì‚¬ìš©ì ì…ë ¥ì— ëŒ€í•œ ì‹¤ì‹œê°„ ê°ì • ë¶„ì„ ë° GIF ì¶”ì²œì— í™œìš©ëœë‹¤.
```python
torch.save(model_koel.state_dict(), "./koelectra_emotion_model_state_dict.pt")
print("ì €ì¥ ì™„ë£Œ: koelectra_emotion_model_state_dict.pt")
```

## 3. ëª¨ë¸ í‰ê°€

### 1) KoBERT ê¸°ë°˜ ê°ì • ë¶„ë¥˜ ëª¨ë¸
ì´ ë‹¨ê³„ì—ì„œëŠ” í•™ìŠµì´ ì™„ë£Œëœ KoBERT ê°ì • ë¶„ë¥˜ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ì •ëŸ‰ì ìœ¼ë¡œ í‰ê°€í•˜ê³ , ê° ê°ì • í´ë˜ìŠ¤ë³„ë¡œ ì–´ë–¤ ìœ í˜•ì˜ ì˜¤ë¶„ë¥˜ê°€ ë°œìƒí•˜ëŠ”ì§€ ë¶„ì„í•œë‹¤. ì´ë¥¼ í†µí•´ KoBERTê°€ ì–´ë–¤ ê°ì •ì— ê°•ì ì´ ìˆê³ , ì–´ë–¤ ê°ì •ì—ì„œ ìƒëŒ€ì ìœ¼ë¡œ ì·¨ì•½í•œì§€ í™•ì¸í•˜ì—¬ ì´í›„ ë°ì´í„° ë³´ì™„ì´ë‚˜ ëª¨ë¸ ê°œì„  ë°©í–¥ì„ ì„¤ì •í•  ìˆ˜ ìˆë‹¤.

- ë¨¼ì € KoBERT ì „ìš© ì…ë ¥ í¬ë§·ì„ ì‚¬ìš©í•˜ëŠ” BERTDatasetê³¼ í•™ìŠµ ì‹œì— ì‚¬ìš©í–ˆë˜ valid_dataloader_kobertë¥¼ ê·¸ëŒ€ë¡œ í™œìš©í•˜ê³ , ë³„ë„ì˜ í‰ê°€ í•¨ìˆ˜ evaluate_kobert()ë¥¼ ì •ì˜í•˜ì—¬ accuracy ë° macro F1-scoreë¥¼ ê³„ì‚°í•œë‹¤. ì´ í•¨ìˆ˜ëŠ” DataLoaderë¥¼ ìˆœíšŒí•˜ë©´ì„œ ì‹¤ì œ ë¼ë²¨ê³¼ ì˜ˆì¸¡ ë¼ë²¨ì„ ìˆ˜ì§‘í•œ ë’¤, scikit-learnì˜ accuracy_scoreì™€ f1_scoreë¥¼ ì´ìš©í•´ ì „ì²´ ì„±ëŠ¥ì„ ì‚°ì¶œí•˜ê³ , í–¥í›„ í˜¼ë™í–‰ë ¬ê³¼ ë¶„ë¥˜ ë¦¬í¬íŠ¸ ìƒì„±ì„ ìœ„í•´ y_true, y_predë„ í•¨ê»˜ ë°˜í™˜í•œë‹¤.
```python
def evaluate_kobert(model, dataloader, device):
    model.eval()
    preds, trues = [], []

    with torch.no_grad():
        for token_ids, valid_length, segment_ids, label in dataloader:
            token_ids = token_ids.long().to(device)
            segment_ids = segment_ids.long().to(device)
            vlen = valid_length
            label = label.long().to(device)

            logits = model(token_ids, vlen, segment_ids)
            pred = torch.argmax(logits, dim=-1)

            preds.extend(pred.cpu().tolist())
            trues.extend(label.cpu().tolist())

    acc = accuracy_score(trues, preds)
    macro_f1 = f1_score(trues, preds, average='macro')
    return acc, macro_f1, np.array(trues), np.array(preds)


kobert_acc, kobert_f1, y_true_kobert, y_pred_kobert = evaluate_kobert(
    kobert_model,
    valid_dataloader_kobert,
    device_kobert
)

print("=== KoBERT Valid ì„±ëŠ¥ ===")
print("accuracy:", kobert_acc)
print("macro F1:", kobert_f1)
```

- ì´í›„ classification_report()ë¥¼ ì´ìš©í•´ 7ê°œ ê°ì • í´ë˜ìŠ¤(ê³µí¬, ë†€ëŒ, ë¶„ë…¸, ìŠ¬í””, ì¤‘ë¦½, í–‰ë³µ, í˜ì˜¤)ì— ëŒ€í•´ precision, recall, F1-scoreë¥¼ ì¶œë ¥í•¨ìœ¼ë¡œì¨, KoBERTê°€ íŠ¹ì • ê°ì •ì— ë” ê°•í•˜ê±°ë‚˜ ì•½í•œì§€ ì„¸ë¶€ì ìœ¼ë¡œ ë¶„ì„í•  ìˆ˜ ìˆë‹¤. ë˜í•œ confusion_matrix()ë¥¼ ì‚¬ìš©í•´ ì‹¤ì œ ë¼ë²¨ê³¼ ì˜ˆì¸¡ ë¼ë²¨ì˜ ì¡°í•©ì„ í–‰ë ¬ í˜•íƒœë¡œ ì¶œë ¥í•˜ê³ , ì´ë¥¼ Matplotlibì„ í™œìš©í•´ í˜¼ë™í–‰ë ¬ ì´ë¯¸ì§€ë¡œ ì‹œê°í™”í•œë‹¤. ë§ˆì§€ë§‰ìœ¼ë¡œ, classification_report ê²°ê³¼ë¥¼ ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë°›ì•„ ê° ê°ì • í´ë˜ìŠ¤ë³„ F1-scoreë§Œ ì¶”ì¶œí•œ ë’¤, ë§‰ëŒ€ ê·¸ë˜í”„ë¡œ ì‹œê°í™”í•œë‹¤.
```python
sorted_items = sorted(target_classes.items(), key=lambda x: x[1])
target_names_kobert = [k for k, v in sorted_items]

print("\n=== KoBERT ê°ì •ë³„ ë¶„ë¥˜ ë¦¬í¬íŠ¸ ===")
print(classification_report(
    y_true_kobert,
    y_pred_kobert,
    target_names=target_names_kobert
))

cm_kobert = confusion_matrix(y_true_kobert, y_pred_kobert)
print("=== KoBERT í˜¼ë™í–‰ë ¬ (í–‰: ì‹¤ì œ, ì—´: ì˜ˆì¸¡) ===")
print(cm_kobert)

plt.rcParams['axes.unicode_minus'] = False

plt.figure(figsize=(8, 6))
plt.imshow(cm_kobert)
plt.title("KoBERT Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("True")
plt.xticks(ticks=np.arange(len(target_names_kobert)), labels=target_names_kobert, rotation=45)
plt.yticks(ticks=np.arange(len(target_names_kobert)), labels=target_names_kobert)
plt.colorbar()
plt.tight_layout()
plt.show()

report_dict_kobert = classification_report(
    y_true_kobert,
    y_pred_kobert,
    target_names=target_names_kobert,
    output_dict=True
)

labels_kobert = target_names_kobert
f1_scores_kobert = [report_dict_kobert[label]["f1-score"] for label in labels_kobert]

plt.figure(figsize=(10, 5))
plt.bar(labels_kobert, f1_scores_kobert)
plt.title("KoBERT Per-Class F1-score")
plt.xlabel("Emotion")
plt.ylabel("F1-score")
plt.ylim(0, 1.05)
plt.grid(axis="y", linestyle="--", alpha=0.5)
plt.tight_layout()
plt.show()
```

KoBERT ê¸°ë°˜ ê°ì • ë¶„ë¥˜ ëª¨ë¸ì€ ê²€ì¦ ë°ì´í„°ì—ì„œ ì•½ 80.38%ì˜ ì •í™•ë„ì™€ 0.72 ìˆ˜ì¤€ì˜ macro F1-scoreë¥¼ ê¸°ë¡í•˜ë©°, ì „ë°˜ì ìœ¼ë¡œ KoELECTRAì™€ ìœ ì‚¬í•œ ìˆ˜ì¤€ì˜ ì•ˆì •ì ì¸ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ì—ˆë‹¤. ê°ì •ë³„ ì„±ëŠ¥ì„ ì‚´í´ë³´ë©´, â€˜ì¤‘ë¦½â€™, â€˜í–‰ë³µâ€™, â€˜í˜ì˜¤â€™ ê°ì •ì—ì„œ íŠ¹íˆ ìš°ìˆ˜í•œ ê²°ê³¼ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìœ¼ë©°, ê·¸ ì¤‘ì—ì„œë„ â€˜í˜ì˜¤â€™ëŠ” F1-scoreê°€ 0.97ë¡œ ê±°ì˜ ì™„ë²½ì— ê°€ê¹Œìš´ ë¶„ë¥˜ ì„±ëŠ¥ì„ ë³´ì˜€ë‹¤. ë°˜ë©´, â€˜ë†€ëŒâ€™ê³¼ â€˜ê³µí¬â€™, â€˜ë¶„ë…¸â€™ ë“± ì¼ë¶€ ê°ì •ì€ ìƒëŒ€ì ìœ¼ë¡œ ì„±ëŠ¥ì´ ë–¨ì–´ì§€ëŠ” í¸ì´ë‹¤. íŠ¹íˆ â€˜ë†€ëŒâ€™ì˜ F1-scoreëŠ” 0.44, â€˜ê³µí¬â€™ëŠ” 0.62, â€˜ë¶„ë…¸â€™ëŠ” 0.60ìœ¼ë¡œ, ìœ ì‚¬í•œ ì •ì„œì  í†¤ì„ ê°€ì§„ ë‹¤ë¥¸ ë¶€ì • ê°ì •(ìŠ¬í””, ë¶„ë…¸, ê³µí¬ ë“±)ê³¼ í˜¼ë™ë˜ëŠ” ì–‘ìƒì´ ë“œëŸ¬ë‚œë‹¤.

### 2) KoELECTRA ê¸°ë°˜ ê°ì • ë¶„ë¥˜ ëª¨ë¸
ì´ ë‹¨ê³„ì—ì„œëŠ” í•™ìŠµì´ ì™„ë£Œëœ KoELECTRA ê°ì • ë¶„ë¥˜ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ì •ëŸ‰ì ìœ¼ë¡œ í‰ê°€í•˜ê³ , ê° ê°ì • í´ë˜ìŠ¤ë³„ë¡œ ì–´ë–¤ íŒ¨í„´ì˜ ì˜¤ë¶„ë¥˜ê°€ ë°œìƒí•˜ëŠ”ì§€ ë¶„ì„í•œë‹¤.

- ë¨¼ì € EmotionDatasetì„ í™œìš©í•´ í•™ìŠµ ë°ì´í„°ì™€ ê²€ì¦ ë°ì´í„°ë¥¼ ê°ê° train_set_koel, valid_set_koelë¡œ êµ¬ì„±í•œ ë’¤, ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ DataLoaderë¥¼ ìƒì„±í•œë‹¤. ì´ë¥¼ í†µí•´ í•™ìŠµ ë°ì´í„°(train)ì™€ ê²€ì¦ ë°ì´í„°(valid) ê°ê°ì— ëŒ€í•´ ì„±ëŠ¥ì„ ì¸¡ì •í•˜ê³ , results_df ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ì •ë¦¬í•´ í•œëˆˆì— ë¹„êµí•  ìˆ˜ ìˆë„ë¡ í•œë‹¤.
```python
def evaluate_with_preds(model, dataloader, device):
    return evaluate_hf(model, dataloader, device)

max_len = 64
batch_size = 32

train_set_koel = EmotionDataset(train_df['sentence'], train_df['emotion'], tokenizer_koel, max_len=max_len)
valid_set_koel = EmotionDataset(valid_df['sentence'], valid_df['emotion'], tokenizer_koel, max_len=max_len)

train_loader_koel = DataLoader(train_set_koel, batch_size=batch_size, shuffle=False)
valid_loader_koel = DataLoader(valid_set_koel, batch_size=batch_size, shuffle=False)

train_acc_koel, train_f1_koel, _, _ = evaluate_with_preds(model_koel, train_loader_koel, device)
valid_acc_koel, valid_f1_koel, y_true_koel, y_pred_koel = evaluate_with_preds(model_koel, valid_loader_koel, device)

results_df = pd.DataFrame([
    ["KoELECTRA-train", train_acc_koel, train_f1_koel],
    ["KoELECTRA-valid", valid_acc_koel, valid_f1_koel],
], columns=["split", "accuracy", "macro_f1"])

print("=== KoELECTRA splitë³„ ì„±ëŠ¥ ===")
print(results_df)
```

- ì¶”ê°€ë¡œ, classification_report()ë¥¼ ì‚¬ìš©í•´ 7ê°œ ê°ì • í´ë˜ìŠ¤(ê³µí¬, ë†€ëŒ, ë¶„ë…¸, ìŠ¬í””, ì¤‘ë¦½, í–‰ë³µ, í˜ì˜¤)ì— ëŒ€í•´ precision, recall, F1-scoreë¥¼ ì¶œë ¥í•¨ìœ¼ë¡œì¨, ì–´ë–¤ ê°ì •ì—ì„œ ìƒëŒ€ì ìœ¼ë¡œ ì„±ëŠ¥ì´ ë–¨ì–´ì§€ëŠ”ì§€ ì„¸ë¶€ì ìœ¼ë¡œ í™•ì¸í•  ìˆ˜ ìˆë‹¤. confusion_matrix()ëŠ” ì‹¤ì œ ë¼ë²¨ê³¼ ì˜ˆì¸¡ ê²°ê³¼ì˜ ì¡°í•©ì„ í–‰ë ¬ í˜•íƒœë¡œ ì œê³µí•˜ë©°, ì´ë¥¼ Matplotlibì„ ì´ìš©í•´ ì‹œê°í™”í•¨ìœ¼ë¡œì¨ ì–´ë–¤ ê°ì •ì´ ë‹¤ë¥¸ ê°ì •ìœ¼ë¡œ ìì£¼ ì˜¤ë¶„ë¥˜ë˜ëŠ”ì§€ë¥¼ ì§ê´€ì ìœ¼ë¡œ íŒŒì•…í•  ìˆ˜ ìˆë‹¤. ë§ˆì§€ë§‰ìœ¼ë¡œ, classification_reportì˜ ê²°ê³¼ë¥¼ ê° ê°ì • í´ë˜ìŠ¤ë³„ F1-scoreì˜ ë§‰ëŒ€ ê·¸ë˜í”„ë¡œ ì‹œê°í™”í•œë‹¤. ì´ ê·¸ë˜í”„ëŠ” ëª¨ë¸ì´ íŠ¹ì • ê°ì •ì— ëŒ€í•´ ìƒëŒ€ì ìœ¼ë¡œ ê°•í•œì§€ í˜¹ì€ ì•½í•œì§€ë¥¼ í•œëˆˆì— ë³´ì—¬ì£¼ë©°, ì´í›„ ë°ì´í„° ë³´ì™„ì´ë‚˜ ëª¨ë¸ ê°œì„  ë°©í–¥ì„ ì„¤ê³„í•˜ëŠ” ë° ì¤‘ìš”í•œ ê¸°ì¤€ìœ¼ë¡œ í™œìš©ë  ìˆ˜ ìˆë‹¤.
```python
target_names = ["ê³µí¬", "ë†€ëŒ", "ë¶„ë…¸", "ìŠ¬í””", "ì¤‘ë¦½", "í–‰ë³µ", "í˜ì˜¤"]

print("\n=== KoELECTRA ê°ì •ë³„ ë¶„ë¥˜ ë¦¬í¬íŠ¸ ===")
print(classification_report(y_true_koel, y_pred_koel, target_names=target_names))

cm_koel = confusion_matrix(y_true_koel, y_pred_koel)

print("=== KoELECTRA í˜¼ë™í–‰ë ¬ (í–‰: ì‹¤ì œ, ì—´: ì˜ˆì¸¡) ===")
print(cm_koel)

plt.figure(figsize=(8, 6))
plt.imshow(cm_koel)
plt.title("KoELECTRA Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("True")
plt.xticks(ticks=np.arange(len(target_names)), labels=target_names, rotation=45)
plt.yticks(ticks=np.arange(len(target_names)), labels=target_names)
plt.colorbar()
plt.tight_layout()
plt.show()

report_dict_koel = classification_report(
    y_true_koel,
    y_pred_koel,
    target_names=target_names,
    output_dict=True
)

f1_scores_koel = [report_dict_koel[label]["f1-score"] for label in target_names]

plt.figure(figsize=(10, 5))
plt.bar(target_names, f1_scores_koel)
plt.title("KoELECTRA Per-Class F1-score")
plt.xlabel("Emotion")
plt.ylabel("F1-score")
plt.ylim(0, 1.05)
plt.grid(axis="y", linestyle="--", alpha=0.5)
plt.tight_layout()
plt.show()
```

KoELECTRA ê°ì • ë¶„ë¥˜ ëª¨ë¸ì€ ì „ì²´ ê²€ì¦ ë°ì´í„° ê¸°ì¤€ ì•½ 81%ì˜ ì •í™•ë„ì™€ 0.72ì˜ macro F1-scoreë¥¼ ë³´ì—¬, 7ê°œ ê°ì • í´ë˜ìŠ¤ ì „ë°˜ì— ê±¸ì³ ë¹„êµì  ì•ˆì •ì ì´ê³  ê· í˜• ì¡íŒ ì„±ëŠ¥ì„ í™•ì¸í•  ìˆ˜ ìˆì—ˆë‹¤. íŠ¹íˆ â€˜ì¤‘ë¦½â€™, â€˜í–‰ë³µâ€™, â€˜í˜ì˜¤â€™ì™€ ê°™ì´ í‘œí˜„ì  íŠ¹ì„±ì´ ëšœë ·í•˜ê±°ë‚˜ ë°ì´í„°ê°€ ì¶©ë¶„í•œ í´ë˜ìŠ¤ì—ì„œëŠ” ë†’ì€ ì •ë°€ë„ì™€ ì¬í˜„ìœ¨ì„ ë³´ì´ë©° ê°•í•œ ë¶„ë¥˜ ì„±ëŠ¥ì„ ë‚˜íƒ€ëƒˆë‹¤. ë°˜ë©´, â€˜ë†€ëŒâ€™ê³¼ ê°™ì€ ì¼ë¶€ ê°ì • í´ë˜ìŠ¤ëŠ” ì¬í˜„ìœ¨ì´ ë‚®ì•„ ë‹¤ë¥¸ ê°ì •(íŠ¹íˆ ì¤‘ë¦½)ìœ¼ë¡œ ì˜¤ë¶„ë¥˜ë˜ëŠ” ê²½í–¥ì´ ìˆì—ˆë‹¤.

### 3) KoBERT, KoELECTRA ë‘ ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ

í•™ìŠµëœ ë‘ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ì¢…í•©í•´ ë´¤ì„ ë•Œ, KoELECTRAëŠ” ê²€ì¦ ì •í™•ë„ 81.16%, macro F1-score 0.715ë¥¼ ê¸°ë¡í–ˆìœ¼ë©°, KoBERTëŠ” ì •í™•ë„ 80.38%, macro F1-score 0.723ë¥¼ ë³´ì˜€ë‹¤. ì •í™•ë„ëŠ” KoELECTRAê°€ ì•½ê°„ ì•ì„œì§€ë§Œ, overall class ê· í˜•ì„ ë°˜ì˜í•˜ëŠ” macro F1-scoreì—ì„œëŠ” KoBERTê°€ ì¡°ê¸ˆ ë” ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì˜€ë‹¤.

<p align="center">
  <img src="images/KoBERT F1-score.png" width="350">
  <img src="images/KoELECTRA F1-score.png" width="350">
</p>


## 4. ê°ì • + í‚¤ì›Œë“œ ê¸°ë°˜ GIF ì¶”ì²œ íŒŒì´í”„ë¼ì¸

ëª¨ë¸ í•™ìŠµê³¼ í‰ê°€ê°€ ëë‚œ ë’¤ì—ëŠ”, í•™ìŠµëœ ê°ì • ë¶„ë¥˜ ëª¨ë¸ì„ ì‹¤ì œ ì„œë¹„ìŠ¤ í˜•íƒœë¡œ í™œìš©í•˜ê¸° ìœ„í•´ **(1) ê°ì • ì˜ˆì¸¡ â†’ (2) í‚¤ì›Œë“œ ì¶”ì¶œ â†’ (3) GIF ì¶”ì²œ**ìœ¼ë¡œ ì´ì–´ì§€ëŠ” íŒŒì´í”„ë¼ì¸ì„ êµ¬í˜„í–ˆë‹¤. ì´ë•Œ ê°ì • ë¶„ë¥˜ ëª¨ë¸ì€ KoBERT ë˜ëŠ” KoELECTRAë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆê³ , í‚¤ì›Œë“œ ì¶”ì¶œì€ ê³µí†µì ìœ¼ë¡œ KoNLPyì˜ Okt í˜•íƒœì†Œ ë¶„ì„ê¸°ë¥¼ ì‚¬ìš©í–ˆë‹¤.

### 1) KoBERT ê¸°ë°˜ ê°ì • + í‚¤ì›Œë“œ ì¶”ì¶œ

ë¨¼ì €, í•™ìŠµëœ KoBERT ëª¨ë¸ê³¼ BERTSentenceTransformë¥¼ ì´ìš©í•´ ì…ë ¥ ë¬¸ì¥ì— ëŒ€í•´ ê°ì •ì„ ì˜ˆì¸¡í•˜ëŠ” predict_emotion_kobert() í•¨ìˆ˜ë¥¼ ì •ì˜í•˜ê³ , ê°ì • ì˜ˆì¸¡ ê²°ê³¼ì— í˜•íƒœì†Œ ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œì„ ê²°í•©í•œ extract_keywords_with_emotion_kobert() í•¨ìˆ˜ë¥¼ ì •ì˜í–ˆë‹¤.

- **1ë‹¨ê³„:** KoBERTë¡œ ê°ì •ì„ ì˜ˆì¸¡
- **2ë‹¨ê³„:** Okt í˜•íƒœì†Œ ë¶„ì„ê¸°ë¡œ í˜•íƒœì†Œ ì¶”ì¶œ (ëª…ì‚¬, í˜•ìš©ì‚¬, ë™ì‚¬ë§Œ í›„ë³´ í‚¤ì›Œë“œë¡œ ì‚¬ìš©, ë¶ˆìš©ì–´ ì œê±°)
- **3ë‹¨ê³„:** ìˆœì„œë¥¼ ìœ ì§€í•œ ì±„ë¡œ ì¤‘ë³µ í‚¤ì›Œë“œë¥¼ ì œê±°í•˜ê³ , ìµœì¢…ì ìœ¼ë¡œ {
    "sentence": ì›ë³¸ ë¬¸ì¥,
    "emotion_label": ì˜ˆì¸¡ ê°ì •,
    "emotion_prob": ê°ì • í™•ë¥ ,
    "keywords": ['ë‚ ì”¨', 'ì¶¥ë‹¤', 'ìš°ìš¸í•˜ë‹¤', ...], ...} í˜•íƒœì˜ ë”•ì…”ë„ˆë¦¬ ë°˜í™˜

```python
def predict_emotion_kobert(sentence, model, tokenizer, id2label, max_len=100):
    device = next(model.parameters()).device
    model.eval()

    transform = BERTSentenceTransform(
        tokenizer,
        max_seq_length=max_len,
        pad=True,
        pair=False
    )

    input_ids, valid_length, segment_ids = transform([sentence])
    valid_length_int = int(valid_length)

    input_ids = torch.from_numpy(input_ids).unsqueeze(0).to(device)
    segment_ids = torch.from_numpy(segment_ids).unsqueeze(0).to(device)
    valid_length = torch.tensor([valid_length_int], dtype=torch.int64).to(device)

    with torch.no_grad():
        logits = model(input_ids, valid_length, segment_ids)
        probs = torch.softmax(logits, dim=-1)
        pred_id = int(torch.argmax(probs, dim=-1).cpu().numpy())
        pred_prob = float(probs[0, pred_id].cpu().numpy())

    pred_label = id2label[pred_id]

    return pred_id, pred_label, pred_prob

def extract_keywords_with_emotion_kobert(
    sentence,
    model,
    tokenizer,
    id2label,
    okt,
    max_len=100
):

    # 1) KoBERT ê°ì • ì˜ˆì¸¡
    pred_id, pred_label, pred_prob = predict_emotion_kobert(
        sentence,
        model,
        tokenizer,
        id2label,
        max_len=max_len
    )

    # 2) OKT í˜•íƒœì†Œ ë¶„ì„
    morphs = okt.pos(sentence, stem=True)

    key_pos = {'Noun', 'Adjective', 'Verb'}
    stopwords = {'í•˜ë‹¤', 'ë˜ë‹¤', 'ìˆë‹¤', 'ì •ë§', 'ì§„ì§œ', 'ê·¼ë°'}

    keywords = []
    for word, pos in morphs:
        if pos in key_pos and len(word) > 1 and word not in stopwords:
            keywords.append(word)

    # ì¤‘ë³µ ì œê±°
    seen = set()
    uniq_keywords = []
    for w in keywords:
        if w not in seen:
            seen.add(w)
            uniq_keywords.append(w)

    return {
        "sentence": sentence,
        "emotion_id": pred_id,
        "emotion_label": pred_label,
        "emotion_prob": pred_prob,
        "keywords": uniq_keywords,
        "morphs_raw": morphs
    }

sentence = "ìš”ì¦˜ ë‚ ì”¨ë„ ì¶¥ê³  ê¸°ë¶„ë„ ì¢€ ìš°ìš¸í•´."

res_kobert = extract_keywords_with_emotion_kobert(
    sentence,
    kobert_model,      # í•™ìŠµëœ KoBERT ë¶„ë¥˜ê¸°
    tokenizer_kobert,  # KoBERT í† í¬ë‚˜ì´ì €
    id2label,
    okt
)
```

### 2) KoELECTRA ê¸°ë°˜ ê°ì • + í‚¤ì›Œë“œ ì¶”ì¶œ
KoELECTRA ê¸°ë°˜ íŒŒì´í”„ë¼ì¸ë„ êµ¬ì¡°ëŠ” ê±°ì˜ ìœ ì‚¬í•˜ë˜, ì…ë ¥ í¬ë§·ì´ HuggingFace ê¸°ë³¸ í¬ë§·(input_ids, attention_mask, token_type_ids)ì´ë¼ëŠ” ì ì´ ë‹¤ë¥´ë‹¤.

```python
def predict_emotion_koelectra(sentence, model, tokenizer, id2label, max_len=64):
    model.eval()
    device = next(model.parameters()).device

    encoded = tokenizer(
        sentence,
        return_tensors='pt',
        truncation=True,
        padding='max_length',
        max_length=max_len
    )
    encoded = {k: v.to(device) for k, v in encoded.items()}

    with torch.no_grad():
        outputs = model(**encoded)
        logits = outputs.logits
        probs = torch.softmax(logits, dim=-1)[0]
        pred_id = int(torch.argmax(probs))

    pred_label = id2label[pred_id]
    return pred_id, pred_label, probs.cpu().numpy()


def extract_keywords_with_emotion_koelectra(
    sentence,
    model,
    tokenizer,
    id2label,
    okt,
    max_len=64
):
    # 1) ê°ì • ì˜ˆì¸¡ (KoELECTRA)
    pred_id, pred_label, probs = predict_emotion_koelectra(
        sentence,
        model,
        tokenizer,
        id2label,
        max_len=max_len
    )

    # 2) í˜•íƒœì†Œ ë¶„ì„
    morphs = okt.pos(sentence, stem=True)

    key_pos = {'Noun', 'Adjective', 'Verb'}
    stopwords = {'í•˜ë‹¤', 'ë˜ë‹¤', 'ìˆë‹¤', 'ê·¸ëƒ¥', 'ì •ë§', 'ì§„ì§œ'}

    keywords = []
    for word, pos in morphs:
        if pos in key_pos and len(word) > 1 and word not in stopwords:
            keywords.append(word)

    # ì¤‘ë³µ ì œê±°
    seen = set()
    uniq_keywords = []
    for w in keywords:
        if w not in seen:
            seen.add(w)
            uniq_keywords.append(w)

    return {
        "sentence": sentence,
        "emotion_id": pred_id,
        "emotion_label": pred_label,
        "emotion_probs": probs,
        "keywords": uniq_keywords,
        "morphs_raw": morphs
    }
okt = Okt()

sentence = "ë‚ ì´ ë„ˆë¬´ ì¶”ì›Œì ¸ì„œ ê°ê¸°ê±¸ë¦¬ê¸° ì‰¬ì›Œ."

res_koel = extract_keywords_with_emotion_koelectra(
    sentence,
    model_koel,       # KoELECTRA í•™ìŠµëœ ëª¨ë¸
    tokenizer_koel,   # KoELECTRA í† í¬ë‚˜ì´ì €
    id2label,
    okt
)
```
### 3) GIPHY APIë¥¼ ì´ìš©í•œ GIF ì¶”ì²œ
ë§ˆì§€ë§‰ìœ¼ë¡œ, ìœ„ì—ì„œ ì–»ì€ emotion_label, keywords ë¥¼ ì´ìš©í•˜ì—¬ GIPHYì—ì„œ ì‹¤ì œ GIFë¥¼ ê²€ìƒ‰í•˜ê³  ì¶”ì²œí•˜ëŠ” ë¡œì§ì„ êµ¬í˜„í–ˆë‹¤. ìµœì¢…ì ìœ¼ë¡œ GIPHYì— ê²€ìƒ‰ì— ì‚¬ìš©í•  í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ëŠ”:
- **Okt ê¸°ë°˜ í‚¤ì›Œë“œ** (ì˜ˆ: ë‚ ì”¨, ê°ê¸°, ì¶¥ë‹¤ ë“±)
- **í•œêµ­ì–´ ê°ì • ë ˆì´ë¸”** (ì˜ˆ: ìŠ¬í””)
- **ì˜ì–´ ê°ì • ë ˆì´ë¸”** (ì˜ˆ: sad)

ìœ„ ì„¸ ìš”ì†Œë¥¼ í•©ì³ì„œ êµ¬ì„±ëœë‹¤. ì¦‰, `["ë‚ ì”¨", "ê°ê¸°", "ì¶¥ë‹¤", "ìŠ¬í””", "sad"]` ì™€ ê°™ì€ í˜•íƒœê°€ ë˜ê²Œ ëœë‹¤.

```python
GIPHY_URL = "https://api.giphy.com/v1/gifs/search"
API_KEY = "P9SzTTpADkMNu3XjLkzRyMxVGm3MyKDQ"

emotion_to_en = {
    'ê³µí¬': 'scared',
    'ë†€ëŒ': 'surprised',
    'ë¶„ë…¸': 'angry',
    'ìŠ¬í””': 'sad',
    'ì¤‘ë¦½': 'neutral',
    'í–‰ë³µ': 'happy',
    'í˜ì˜¤': 'disgust'
}

keywords_ko = res_kobert["keywords"][:]

emo_ko = res_kobert["emotion_label"]
keywords_ko.append(emo_ko)

emo_en = emotion_to_en.get(emo_ko, "emotion")
keywords_ko.append(emo_en)

seen_kw = set()
keywords_final = []
for w in keywords_ko:
    if w not in seen_kw:
        seen_kw.add(w)
        keywords_final.append(w)

print("ìµœì¢… GIPHY ê²€ìƒ‰ í‚¤ì›Œë“œ:", keywords_final)
```

- ì´í›„ ê° í‚¤ì›Œë“œì— ëŒ€í•´ GIPHY APIë¥¼ í˜¸ì¶œí•˜ê³ , ì´í›„ ê° í‚¤ì›Œë“œì— ëŒ€í•´ GIPHY APIë¥¼ í˜¸ì¶œí•œ í›„ ìµœëŒ€ 25ê°œì˜ GIFë¥¼ ê²€ìƒ‰í•˜ê³ , ê·¸ ì¤‘ ìµœëŒ€ 3ê°œë¥¼ ëœë¤ìœ¼ë¡œ ì„ íƒí•˜ì—¬ ì „ì²´ ì¶”ì²œ ë¦¬ìŠ¤íŠ¸ë¥¼ ë§Œë“ ë‹¤.

```python
all_urls = []

for kw in keywords_final:
    params = parse.urlencode({
        "q": kw,
        "api_key": API_KEY,
        "limit": "25"
    })

    try:
        with request.urlopen(f"{GIPHY_URL}?{params}") as response:
            data = json.loads(response.read())

        results = data.get("data", [])
        print(f"í‚¤ì›Œë“œ '{kw}' ê²€ìƒ‰ ê²°ê³¼:", len(results))

        if not results:
            continue

        # ê° í‚¤ì›Œë“œ ìµœëŒ€ 3ê°œì”© ì„ íƒ
        n = len(results)
        target_n = min(3, n)
        idxs = random.sample(range(n), target_n)

        for i in idxs:
            all_urls.append(results[i]["images"]["downsized"]["url"])

    except Exception as e:
        print(f"í‚¤ì›Œë“œ '{kw}' ê²€ìƒ‰ ì¤‘ ì—ëŸ¬:", e)

if not all_urls:
    print("GIF ì¶”ì²œ ì‹¤íŒ¨ ğŸ˜¢ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
else:
    # ì „ì²´ ì¤‘ë³µ ì œê±°
    all_urls = list(dict.fromkeys(all_urls))

    # ë” ì´ìƒ ì „ì²´ ê°œìˆ˜ ì œí•œ ì•ˆ ê±¸ê³ , ì „ë¶€ ë³´ì—¬ì£¼ê¸°
    picked = [re.sub(r"media\d", "i", url) for url in all_urls]

    print(f"[ì´ {len(all_urls)}ê°œ ì¶”ì²œ]")
    ipyplot.plot_images(picked, img_width=200, show_url=False)
```
ì •ë¦¬í•˜ë©´, GIF ì¶”ì²œ ë¡œì§ì€ ë‹¤ìŒê³¼ ê°™ì€ ì „ëµì„ ë”°ë¥¸ë‹¤.
- **ëª¨ë¸ì´ ì´í•´í•œ ê°ì • + í˜•íƒœì†Œ ë¶„ì„ìœ¼ë¡œ ì¶”ì¶œí•œ í‚¤ì›Œë“œ**ë¥¼ ëª¨ë‘ í™œìš©
- ê°ì •(í•œêµ­ì–´/ì˜ì–´) + í‚¤ì›Œë“œ ì¡°í•©ìœ¼ë¡œ ì—¬ëŸ¬ ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ ë§Œë“¤ì–´ GIPHYë¥¼ ì¡°íšŒ
- ê° ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ëª‡ ê°œì”© ëœë¤ ìƒ˜í”Œë§í•˜ì—¬,
    - íŠ¹ì • í‚¤ì›Œë“œì— ì¹˜ìš°ì¹˜ì§€ ì•Šìœ¼ë©´ì„œ
    - ì ë‹¹í•œ ë‹¤ì–‘ì„±ì„ ê°€ì§„ GIF ë¬¶ìŒì„ êµ¬ì„±
- ìµœì¢…ì ìœ¼ë¡œ ì¤‘ë³µ ì œê±° í›„ ì‚¬ìš©ìì—ê²Œ GIF ë¦¬ìŠ¤íŠ¸ë¥¼ ë³´ì—¬ì¤€ë‹¤.

ì´ ë¡œì§ì€ ì´í›„ app.py (Streamlit ì•±)ì—ì„œ ë²„íŠ¼ í´ë¦­ â†’ ê°ì • + í‚¤ì›Œë“œ ì¶”ì¶œ â†’ GIPHY ê²€ìƒ‰ â†’ ì´ë¯¸ì§€ ë Œë”ë§ìœ¼ë¡œ í†µí•©ë˜ì–´, ì‚¬ìš©ìê°€ ì‹¤ì œë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ì„œë¹„ìŠ¤ì˜ í˜•íƒœë¡œ êµ¬í˜„ëœë‹¤.

## 5. Streamlit ê¸°ë°˜ ì›¹ ë°ëª¨ ì•±(app.py) êµ¬í˜„

ì•ê¹Œì§€ì˜ ê³¼ì •ì—ì„œ ìš°ë¦¬ëŠ”
- í†µí•© ë°ì´í„°ì…‹ êµ¬ì¶• ë° ì „ì²˜ë¦¬
- KoBERT / KoELECTRA ê°ì • ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ ë° í‰ê°€
- ê°ì • + í‚¤ì›Œë“œ ê¸°ë°˜ GIPHY GIF ì¶”ì²œ ë¡œì§ ì„¤ê³„

ë¥¼ ì™„ë£Œí–ˆë‹¤. ì´ì œ ì´ëŸ¬í•œ ê¸°ëŠ¥ì„ ì‹¤ì œ ì‚¬ìš©ìê°€ ì²´í—˜í•  ìˆ˜ ìˆë„ë¡, Streamlit ê¸°ë°˜ ì›¹ ë°ëª¨ ì•±ì„ êµ¬í˜„í–ˆë‹¤. ì´ ì•±ì€ ë¡œì»¬/ì„œë²„ì—ì„œ ì‹¤í–‰ ê°€ëŠ¥í•œ ê°„ë‹¨í•œ ì›¹ ì¸í„°í˜ì´ìŠ¤ë¡œ, ì‚¬ìš©ìê°€ í•œêµ­ì–´ ë¬¸ì¥ì„ ì…ë ¥í•˜ë©´ **ë¬¸ì¥ â†’ ê°ì • ì˜ˆì¸¡(KoBERT/KoELECTRA) â†’ í‚¤ì›Œë“œ ì¶”ì¶œ(OKT) â†’ GIPHY ê²€ìƒ‰ & ì¶”ì²œ**ê¹Œì§€ì˜ ì „ì²´ íŒŒì´í”„ë¼ì¸ì„ í•œ ë²ˆì— ê²½í—˜í•  ìˆ˜ ìˆë„ë¡ í•œë‹¤.

app.pyëŠ” í¬ê²Œ ë‹¤ìŒê³¼ ê°™ì´ ë„¤ ë¶€ë¶„ìœ¼ë¡œ êµ¬ì„±ëœë‹¤.
- **ëª¨ë¸ ìœ í‹¸ ë° ê°ì • + í‚¤ì›Œë“œ ì¶”ì¶œ í•¨ìˆ˜ ì •ì˜**
    - KoBERT/KoELECTRA
    - predict_emotion_, extract_keywords_with_emotion_
- **GIPHY API ê¸°ë°˜ GIF ì¶”ì²œ í•¨ìˆ˜**
    - ê°ì • ë ˆì´ë¸” + í‚¤ì›Œë“œë¥¼ ì´ìš©í•´ GIPHYì—ì„œ GIF ê²€ìƒ‰ & ìƒ˜í”Œë§
- **ëª¨ë¸/í† í¬ë‚˜ì´ì €/í˜•íƒœì†Œ ë¶„ì„ê¸° ë¡œë”©**
    - KoBERT, KoELECTRA ëª¨ë¸ ê°€ì¤‘ì¹˜(`.pt`)íŒŒì¼ ë¡œë“œ
    - HuggingFaceì—ì„œ ì‚¬ì „í•™ìŠµ ëª¨ë¸ + í† í¬ë‚˜ì´ì € ë¶ˆëŸ¬ì˜¤ê¸°
    - Okt ì´ˆê¸°í™”
- **Streamlit UI (main í•¨ìˆ˜)**
    - ì‚¬ì´ë“œë°”(KoBERT/KoELECTRA ëª¨ë¸ ì„ íƒ, ìµœëŒ€ ê¸¸ì´, GIPHY API í‚¤)
    - ë©”ì¸ ì±„íŒ… UI
    - GIF ì¶”ì²œ ê²°ê³¼ ê·¸ë¦¬ë“œ
    - ì‚¬ì´ë“œë°”ì— ê°ì • ë¶„ì„ ê²°ê³¼ ë° í‚¤ì›Œë“œ ìš”ì•½
```python
import os
import random
import json
import re

import torch
from torch import nn

import streamlit as st
import requests

from transformers import BertModel, AutoTokenizer, AutoModelForSequenceClassification
from kobert_tokenizer import KoBERTTokenizer
from konlpy.tag import Okt

import numpy as np
import html

############################################
# 0. ëœë¤ ì‹œë“œ
############################################
def set_seed(seed=42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)

set_seed(42)

############################################
# 1. BERT ê´€ë ¨ ìœ í‹¸
############################################

class BERTSentenceTransform:
    def __init__(self, tokenizer, max_seq_length, pad=True, pair=False):
        self._tokenizer = tokenizer
        self._max_seq_length = max_seq_length
        self._pad = pad
        self._pair = pair

        self.cls_token = self._tokenizer.cls_token or '[CLS]'
        self.sep_token = self._tokenizer.sep_token or '[SEP]'
        self.pad_token_id = self._tokenizer.pad_token_id

    def _truncate_seq_pair(self, tokens_a, tokens_b, max_length):
        while True:
            total_length = len(tokens_a) + len(tokens_b)
            if total_length <= max_length:
                break
            if len(tokens_a) > len(tokens_b):
                tokens_a.pop()
            else:
                tokens_b.pop()

    def __call__(self, line):
        text_a = line[0]
        if self._pair:
            assert len(line) == 2
            text_b = line[1]

        tokens_a = self._tokenizer.tokenize(text_a)
        tokens_b = None

        if self._pair:
            tokens_b = self._tokenizer.tokenize(text_b)

        if tokens_b:
            self._truncate_seq_pair(tokens_a, tokens_b,
                                    self._max_seq_length - 3)
        else:
            if len(tokens_a) > self._max_seq_length - 2:
                tokens_a = tokens_a[:self._max_seq_length - 2]

        tokens = []
        tokens.append(self.cls_token)
        tokens.extend(tokens_a)
        tokens.append(self.sep_token)

        segment_ids = [0] * len(tokens)

        if tokens_b:
            tokens.extend(tokens_b)
            tokens.append(self.sep_token)
            segment_ids.extend([1] * (len(tokens) - len(segment_ids)))

        input_ids = self._tokenizer.convert_tokens_to_ids(tokens)
        valid_length = len(input_ids)

        if self._pad:
            padding_length = self._max_seq_length - valid_length
            input_ids.extend([self.pad_token_id] * padding_length)
            segment_ids.extend([0] * padding_length)

        return (np.array(input_ids, dtype='int32'),
                np.array(valid_length, dtype='int32'),
                np.array(segment_ids, dtype='int32'))


class BERTClassifier(nn.Module):
    def __init__(self,
                 bert,
                 hidden_size=768,
                 num_classes=7,
                 dr_rate=None):
        super(BERTClassifier, self).__init__()
        self.bert = bert
        self.dr_rate = dr_rate

        self.classifier = nn.Linear(hidden_size, num_classes)
        if dr_rate:
            self.dropout = nn.Dropout(p=dr_rate)

    def gen_attention_mask(self, token_ids, valid_length):
        attention_mask = torch.zeros_like(token_ids)
        for i, v in enumerate(valid_length):
            attention_mask[i][:v] = 1
        return attention_mask.float()

    def forward(self, token_ids, valid_length, segment_ids):
        attention_mask = self.gen_attention_mask(token_ids, valid_length)
        _, pooler = self.bert(
            input_ids=token_ids,
            token_type_ids=segment_ids.long(),
            attention_mask=attention_mask.to(token_ids.device),
            return_dict=False
        )
        if self.dr_rate:
            out = self.dropout(pooler)
        else:
            out = pooler
        return self.classifier(out)

############################################
# 2. KoBERT ê°ì • ì˜ˆì¸¡ + í‚¤ì›Œë“œ ì¶”ì¶œ í•¨ìˆ˜
############################################

target_classes = {
    'ê³µí¬': 0,
    'ë†€ëŒ': 1,
    'ë¶„ë…¸': 2,
    'ìŠ¬í””': 3,
    'ì¤‘ë¦½': 4,
    'í–‰ë³µ': 5,
    'í˜ì˜¤': 6
}
idx2label = {v: k for k, v in target_classes.items()}

def predict_emotion_kobert(sentence, model, tokenizer, transform, idx2label, max_len=100):
    device = next(model.parameters()).device
    model.eval()

    input_ids, valid_length, segment_ids = transform([sentence])
    valid_length_int = int(valid_length)

    input_ids = torch.from_numpy(input_ids).unsqueeze(0).to(device)
    segment_ids = torch.from_numpy(segment_ids).unsqueeze(0).to(device)
    valid_length = torch.tensor([valid_length_int], dtype=torch.int64).to(device)

    with torch.no_grad():
        logits = model(input_ids, valid_length, segment_ids)
        probs = torch.softmax(logits, dim=-1)
        pred_id = int(torch.argmax(probs, dim=-1).cpu().numpy())
        pred_prob = float(probs[0, pred_id].cpu().numpy())

    pred_label = idx2label[pred_id]
    return pred_id, pred_label, pred_prob


def extract_keywords_with_emotion_kobert(sentence, model, tokenizer, transform, idx2label, okt, max_len=100):
    # 1) KoBERT ê°ì • ì˜ˆì¸¡
    pred_id, pred_label, pred_prob = predict_emotion_kobert(
        sentence,
        model,
        tokenizer,
        transform,
        idx2label,
        max_len=max_len
    )
    # 2) OKT í˜•íƒœì†Œ ë¶„ì„
    morphs = okt.pos(sentence, stem=True)

    key_pos = {'Noun', 'Adjective', 'Verb'}
    stopwords = {'í•˜ë‹¤', 'ë˜ë‹¤', 'ìˆë‹¤', 'ì •ë§', 'ì§„ì§œ', 'ê·¼ë°'}

    keywords = []
    for word, pos in morphs:
        if pos in key_pos and len(word) > 1 and word not in stopwords:
            keywords.append(word)

    # ì¤‘ë³µ ì œê±°
    seen = set()
    uniq_keywords = []
    for w in keywords:
        if w not in seen:
            seen.add(w)
            uniq_keywords.append(w)

    return {
        "sentence": sentence,
        "emotion_id": pred_id,
        "emotion_label": pred_label,
        "emotion_prob": pred_prob,
        "keywords": uniq_keywords,
        "morphs_raw": morphs
    }

############################################
# 2-1. KoELECTRA ê°ì • ì˜ˆì¸¡ + í‚¤ì›Œë“œ ì¶”ì¶œ í•¨ìˆ˜
############################################

def predict_emotion_koelectra(sentence, model, tokenizer, idx2label, max_len=64):
    model.eval()
    device = next(model.parameters()).device

    encoded = tokenizer(
        sentence,
        return_tensors='pt',
        truncation=True,
        padding='max_length',
        max_length=max_len
    )
    encoded = {k: v.to(device) for k, v in encoded.items()}

    with torch.no_grad():
        outputs = model(**encoded)
        logits = outputs.logits
        probs = torch.softmax(logits, dim=-1)[0]
        pred_id = int(torch.argmax(probs))

    pred_label = idx2label[pred_id]
    pred_prob = float(probs[pred_id].cpu().numpy())

    return pred_id, pred_label, pred_prob


def extract_keywords_with_emotion_koelectra(
    sentence,
    model,
    tokenizer,
    idx2label,
    okt,
    max_len=64
):
    # 1) ê°ì • ì˜ˆì¸¡ (KoELECTRA)
    pred_id, pred_label, pred_prob = predict_emotion_koelectra(
        sentence,
        model,
        tokenizer,
        idx2label,
        max_len=max_len
    )

    # 2) í˜•íƒœì†Œ ë¶„ì„
    morphs = okt.pos(sentence, stem=True)

    key_pos = {'Noun', 'Adjective', 'Verb'}
    stopwords = {'í•˜ë‹¤', 'ë˜ë‹¤', 'ìˆë‹¤', 'ê·¸ëƒ¥', 'ì •ë§', 'ì§„ì§œ'}

    keywords = []
    for word, pos in morphs:
        if pos in key_pos and len(word) > 1 and word not in stopwords:
            keywords.append(word)

    # ì¤‘ë³µ ì œê±°
    seen = set()
    uniq_keywords = []
    for w in keywords:
        if w not in seen:
            seen.add(w)
            uniq_keywords.append(w)

    return {
        "sentence": sentence,
        "emotion_id": pred_id,
        "emotion_label": pred_label,
        "emotion_prob": pred_prob,
        "keywords": uniq_keywords,
        "morphs_raw": morphs
    }


############################################
# 3. GIPHY GIF ì¶”ì²œ í•¨ìˆ˜
############################################

EMOTION_TO_EN = {
    'ê³µí¬': 'scared',
    'ë†€ëŒ': 'surprised',
    'ë¶„ë…¸': 'angry',
    'ìŠ¬í””': 'sad',
    'ì¤‘ë¦½': 'neutral',
    'í–‰ë³µ': 'happy',
    'í˜ì˜¤': 'disgust'
}

def recommend_gifs_from_result(res_kobert, api_key, limit_per_kw=3, total_limit=12):
    giphy_url = "https://api.giphy.com/v1/gifs/search"

    if not api_key:
        return [], "GIPHY API Keyê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."

    keywords_ko = res_kobert["keywords"][:]
    emo_ko = res_kobert["emotion_label"]
    keywords_ko.append(emo_ko)
    emo_en = EMOTION_TO_EN.get(emo_ko, "emotion")
    keywords_ko.append(emo_en)

    seen_kw = set()
    keywords_final = []
    for w in keywords_ko:
        if w not in seen_kw:
            seen_kw.add(w)
            keywords_final.append(w)

    all_urls = []

    for kw in keywords_final:
        params = {
            "q": kw,
            "api_key": api_key,
            "limit": "25",
            "rating": "pg-13"
        }
        try:
            resp = requests.get(giphy_url, params=params, timeout=5)
            if resp.status_code != 200:
                continue
            data = resp.json()
            results = data.get("data", [])

            if not results:
                continue

            n = len(results)
            target_n = min(limit_per_kw, n)
            idxs = random.sample(range(n), target_n)

            for i in idxs:
                url = results[i]["images"]["downsized"]["url"]
                all_urls.append(url)

        except Exception as e:
            print(f"GIPHY ê²€ìƒ‰ ì¤‘ ì—ëŸ¬ (kw={kw}):", e)

    if not all_urls:
        return [], "GIF ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."

    all_urls = list(dict.fromkeys(all_urls))

    if len(all_urls) > total_limit:
        all_urls = random.sample(all_urls, total_limit)

    picked = [re.sub(r"media\d", "i", url) for url in all_urls]

    return picked, f"ì´ {len(picked)}ê°œ GIF ì¶”ì²œ"

############################################
# 4. ëª¨ë¸/í† í¬ë‚˜ì´ì €/OKT ë¡œë“œ (ìºì‹œ)
############################################

@st.cache_resource
def load_kobert_model_and_tokenizer(max_len=100, model_path="./kobert_emotion_model_state_dict.pt"):
    device_type = 'cuda' if torch.cuda.is_available() else 'cpu'
    device = torch.device(device_type)

    tokenizer = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1')
    bertmodel = BertModel.from_pretrained('skt/kobert-base-v1')

    model = BERTClassifier(bertmodel, dr_rate=0.5).to(device)

    if os.path.exists(model_path):
        state_dict = torch.load(model_path, map_location=device)
        model.load_state_dict(state_dict)
    else:
        print(f"ê²½ê³ : {model_path} íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

    transform = BERTSentenceTransform(
        tokenizer,
        max_seq_length=max_len,
        pad=True,
        pair=False
    )

    okt = Okt()

    return model, tokenizer, transform, okt, device

@st.cache_resource
def load_koelectra_model_and_tokenizer(max_len=64, model_path="./koelectra_emotion_model_state_dict.pt"):
    device_type = 'cuda' if torch.cuda.is_available() else 'cpu'
    device = torch.device(device_type)

    tokenizer = AutoTokenizer.from_pretrained("monologg/koelectra-base-v3-discriminator")

    model = AutoModelForSequenceClassification.from_pretrained(
        "monologg/koelectra-base-v3-discriminator",
        num_labels=len(target_classes)
    ).to(device)

    if os.path.exists(model_path):
        state_dict = torch.load(model_path, map_location=device)
        model.load_state_dict(state_dict)
    else:
        print(f"ê²½ê³ : {model_path} íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

    okt = Okt()
    return model, tokenizer, okt, device

############################################
# 5. Streamlit UI
############################################

def inject_chat_css():
    st.markdown("""
    <style>
    .chat-wrapper {
        max-width: 700px;
        margin: 0.5rem 0 1.0rem 0;
    }

    .chat-row {
        display: flex;
        margin-bottom: 0.8rem;
    }

    .chat-bubble {
        padding: 18px 24px;
        border-radius: 26px;
        max-width: 100%;
        font-size: 1.2rem;
        line-height: 1.6;
        word-wrap: break-word;
        word-break: keep-all;
    }

    .chat-bubble.bot {
        background: #f1f0f0;
        color: #111;
        border-bottom-left-radius: 8px;
    }

    .chat-bubble.user {
        margin-left: auto;
        background: linear-gradient(
            to bottom,
            #a855f7,
            #7c3aed
        );
        color: #fff;
        border-bottom-right-radius: 6px;
    }
                            
    .chat-meta {
        font-size: 0.8rem;
        color: #999;
        margin-top: 0.1rem;
        margin-bottom: 0.9rem;
    }

    .keyboard-img {
        opacity: 0.9;
        margin-top: 0.6rem;
    }
    </style>
    """, unsafe_allow_html=True)

def main():
    st.set_page_config(page_title="ê°ì • ë¶„ì„ + GIF ì¶”ì²œ ì„œë¹„ìŠ¤", layout="wide")
    inject_chat_css()
    
    # ===== ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” =====
    if 'sent_once' not in st.session_state:
        st.session_state['sent_once'] = False
    if 'last_result' not in st.session_state:
        st.session_state['last_result'] = None
    
    # ===== ì‚¬ì´ë“œë°”: ì„¤ì • =====
    
    backend = st.sidebar.radio(
        "ì‚¬ìš©í•  ëª¨ë¸",
        ["KoBERT", "KoELECTRA"],
        index=0
    )
    
    max_len = st.sidebar.slider(
        "ìµœëŒ€ í† í° ê¸¸ì´",
        min_value=32,
        max_value=256,
        value=100 if backend == "KoBERT" else 64,
        step=4
    )
    
    giphy_api_key = st.sidebar.text_input(
        "GIPHY API Key",
        "P9SzTTpADkMNu3XjLkzRyMxVGm3MyKDQ",
        type="password"
    )
    
    # ===== ëª¨ë¸ ë¡œë”© =====
    with st.spinner("ëª¨ë¸ ë¡œë”© ì¤‘... (ìµœì´ˆ 1íšŒ)"):
        if backend == "KoBERT":
            model, tokenizer, transform, okt, device = load_kobert_model_and_tokenizer(
                max_len=max_len
            )
        else:
            model, tokenizer, okt, device = load_koelectra_model_and_tokenizer(
                max_len=max_len
            )
            transform = None

    st.session_state['backend'] = backend

    # ===== ë©”ì¸ ì˜ì—­ =====
    st.markdown("## ğŸ“© ê°ì • ë¶„ì„ + GIF ì¶”ì²œ ì„œë¹„ìŠ¤")

    chat_container = st.container()

    # ===== ì…ë ¥ì°½ & ë²„íŠ¼ =====
    default_text = "ìš”ì¦˜ ë‚ ì”¨ë„ ì¶¥ê³  ê¸°ë¶„ë„ ì¢€ ìš°ìš¸í•´."
    col_input, col_btn = st.columns([5, 1])

    with col_input:
        sentence = st.text_input(
            "ë©”ì‹œì§€ ì…ë ¥",
            value=default_text,
            label_visibility="collapsed"
        )
    with col_btn:
        analyze_clicked = st.button("GIF ì¶”ì²œë°›ê¸°")
    
    # 1ë‹¨ê³„: ë²„íŠ¼ í´ë¦­ ì²˜ë¦¬ & ê°ì • ë¶„ì„
    if analyze_clicked:
        if not sentence.strip():
            st.warning("ë¬¸ì¥ì„ ì…ë ¥í•´ ì£¼ì„¸ìš”.")
            return

        st.session_state['sent_once'] = True

        with st.spinner("ê°ì • ë¶„ì„ ì¤‘..."):
            if backend == "KoBERT":
                res = extract_keywords_with_emotion_kobert(
                    sentence,
                    model,
                    tokenizer,
                    transform,
                    idx2label,
                    okt,
                    max_len=max_len
                )
            else:
                res = extract_keywords_with_emotion_koelectra(
                    sentence,
                    model,
                    tokenizer,
                    idx2label,
                    okt,
                    max_len=max_len
                )

        st.session_state['last_result'] = res

    # 2ë‹¨ê³„: ìœ„ìª½ ì±„íŒ… ì˜ì—­ ë Œë”
    with chat_container:
        # 1) ìƒëŒ€ë°© ë§í’ì„ 
        st.markdown("""
        <div class="chat-wrapper">
          <div class="chat-row">
            <div class="chat-bubble bot">
              ì˜¤ëŠ˜ ì–´ë–¤ ì¼ì´ ìˆì—ˆì–´?
            </div>
          </div>
        """, unsafe_allow_html=True)
    
        if st.session_state['last_result'] is not None:
            user_text = html.escape(st.session_state['last_result']["sentence"])
        else:
            user_text = '<span style="opacity:0.4;">ì—¬ê¸°ì— ë‹¹ì‹ ì˜ ë©”ì‹œì§€ê°€ ë“¤ì–´ê°‘ë‹ˆë‹¤</span>'
    
        st.markdown(f"""
          <div class="chat-row">
            <div class="chat-bubble user">
              {user_text}
            </div>
          </div>
        </div> <!-- chat-wrapper end -->
        """, unsafe_allow_html=True)   

    # ===== í‚¤ë³´ë“œ ê·¸ë¦¼ =====
    if not st.session_state['sent_once']:
        try:
            st.image("keyboard.png", width=1000)
        except Exception:
            st.caption("ğŸ’¡ 'keyboard.png' íŒŒì¼ì„ ê°™ì€ í´ë”ì— ë‘ë©´ ì—¬ê¸° í‚¤ë³´ë“œ ê·¸ë¦¼ì´ ëœ¹ë‹ˆë‹¤.")

    # ===== GIF ì¶”ì²œ =====
    if st.session_state['last_result'] is not None:
        res_kobert = st.session_state['last_result']

        st.markdown("---")
        st.markdown("### ğŸ¬ GIF ì¶”ì²œ")

        with st.spinner("GIPHYì—ì„œ GIF ê²€ìƒ‰ ì¤‘..."):
            gif_urls, msg = recommend_gifs_from_result(
                res_kobert,
                giphy_api_key,
                limit_per_kw=3,
                total_limit=12
            )

        st.write(msg)

        if gif_urls:
            cols = st.columns(3)
            for i, url in enumerate(gif_urls):
                with cols[i % 3]:
                    st.image(url, width=250)

    # ===== ì‚¬ì´ë“œë°”: ê°ì • ë¶„ì„ ê²°ê³¼ ì˜ì—­ =====
    st.sidebar.markdown("---")
    st.sidebar.subheader("ğŸ” ê°ì • ë¶„ì„ ê²°ê³¼")

    if st.session_state['last_result'] is not None:
        res = st.session_state['last_result']
        st.sidebar.markdown("**ë¬¸ì¥**")
        st.sidebar.write(res["sentence"])

        st.sidebar.markdown("**ì˜ˆì¸¡ ê°ì •**")
        st.sidebar.markdown(
            f"ğŸ§¾ **{res['emotion_label']}**  "
            f"({res['emotion_prob']*100:.2f}% í™•ì‹ )"
        )

        st.sidebar.markdown("**í‚¤ì›Œë“œ**")
        if res["keywords"]:
            st.sidebar.write(", ".join(res["keywords"]))
        else:
            st.sidebar.write("_í‚¤ì›Œë“œê°€ ì¶”ì¶œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤._")
    else:
        st.sidebar.caption("ì•„ì§ ë¶„ì„ëœ ë¬¸ì¥ì´ ì—†ìŠµë‹ˆë‹¤.")


if __name__ == "__main__":
    main()
```


